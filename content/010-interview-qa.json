{
  "config": {
    "id": "interview-qa",
    "name": "Interview Q&A",
    "description": "Comprehensive technical interview questions and answers covering various domains.",
    "icon": "🎯",
    "enabled": true
  },
  "content": [
    {
      "id": "database-performance-qa",
      "title": "🗄️ Database Performance & Optimization Q&A",
      "sections": [
        {
          "name": "Slow Insert Performance",
          "text": "# Database Insert Performance Issues\n\n**Question:** If a database takes long to insert a new record due to the table having millions of records, what could be the approach to improve the performance?\n\n## **Answer:**\n\n### **Root Causes:**\n- **Index Overhead**: Too many indexes slow down INSERT operations\n- **Lock Contention**: Table-level locks during inserts\n- **Disk I/O**: Slow disk writes due to fragmentation\n- **Constraint Checks**: Foreign key and unique constraint validations\n- **Trigger Execution**: Complex triggers running on each insert\n\n### **Performance Optimization Strategies:**\n\n#### **1. Index Optimization**\n- **Remove Unused Indexes**: Drop indexes that aren't used for queries\n- **Use Partial Indexes**: Create indexes only for frequently queried data\n- **Consider Index Types**: Use appropriate index types (B-tree, Hash, etc.)\n\n#### **2. Batch Operations**\n- **Bulk Inserts**: Use `INSERT INTO ... VALUES (...), (...), (...)`\n- **Batch Processing**: Insert multiple records in single transaction\n- **Prepared Statements**: Reduce parsing overhead\n\n#### **3. Database Configuration**\n- **Increase Buffer Pool**: Allocate more memory for caching\n- **Optimize Log Files**: Use faster storage for transaction logs\n- **Tune Write-Ahead Logging**: Optimize WAL settings\n\n#### **4. Table Design**\n- **Partitioning**: Split large tables into smaller partitions\n- **Denormalization**: Reduce JOIN operations for better performance\n- **Data Types**: Use appropriate data types to reduce storage\n\n#### **5. Hardware Optimization**\n- **SSD Storage**: Use solid-state drives for better I/O performance\n- **RAID Configuration**: Optimize disk array setup\n- **Memory**: Increase RAM for better caching\n\n### **Monitoring & Analysis:**\n- **Query Execution Plans**: Analyze INSERT performance\n- **Index Usage Statistics**: Monitor index utilization\n- **Lock Wait Times**: Identify contention issues\n- **Disk I/O Metrics**: Monitor storage performance",
          "code": "-- Example: Batch Insert Optimization\n-- Instead of multiple single inserts:\nINSERT INTO users (name, email) VALUES ('John', 'john@example.com');\nINSERT INTO users (name, email) VALUES ('Jane', 'jane@example.com');\nINSERT INTO users (name, email) VALUES ('Bob', 'bob@example.com');\n\n-- Use batch insert:\nINSERT INTO users (name, email) VALUES \n    ('John', 'john@example.com'),\n    ('Jane', 'jane@example.com'),\n    ('Bob', 'bob@example.com');\n\n-- Example: Index Optimization\n-- Remove unused indexes\nDROP INDEX idx_unused_column ON large_table;\n\n-- Create partial index for active records only\nCREATE INDEX idx_active_users ON users(email) WHERE status = 'active';\n\n-- Example: Table Partitioning (PostgreSQL)\nCREATE TABLE orders (\n    id SERIAL,\n    order_date DATE,\n    customer_id INT,\n    amount DECIMAL\n) PARTITION BY RANGE (order_date);\n\nCREATE TABLE orders_2023 PARTITION OF orders\n    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');\n\n-- Example: Prepared Statement (MySQL)\nPREPARE stmt FROM 'INSERT INTO users (name, email) VALUES (?, ?)';\nSET @name = 'John', @email = 'john@example.com';\nEXECUTE stmt USING @name, @email;\nDEALLOCATE PREPARE stmt;"
        },
        {
          "name": "Query Optimization",
          "text": "# Database Query Optimization\n\n**Question:** How would you optimize a slow-running query that takes 30 seconds to return results?\n\n## **Answer:**\n\n### **Step 1: Analysis & Profiling**\n- **EXPLAIN PLAN**: Analyze query execution plan\n- **Identify Bottlenecks**: Look for full table scans, expensive joins\n- **Monitor Resources**: Check CPU, memory, and I/O usage\n- **Index Usage**: Verify if indexes are being used effectively\n\n### **Step 2: Query Structure Optimization**\n- **Rewrite Subqueries**: Convert to JOINs where possible\n- **Limit Result Set**: Use WHERE clauses to filter early\n- **Avoid SELECT ***: Only select needed columns\n- **Optimize JOINs**: Use appropriate JOIN types and order\n\n### **Step 3: Index Strategy**\n- **Create Missing Indexes**: Add indexes for WHERE and JOIN columns\n- **Composite Indexes**: Create multi-column indexes for complex queries\n- **Covering Indexes**: Include all needed columns in index\n- **Partial Indexes**: Create indexes for filtered data subsets\n\n### **Step 4: Database Configuration**\n- **Buffer Pool Size**: Increase memory allocation\n- **Query Cache**: Enable and tune query caching\n- **Connection Pooling**: Optimize connection management\n- **Statistics**: Update table statistics regularly\n\n### **Step 5: Application-Level Optimization**\n- **Pagination**: Implement cursor-based pagination\n- **Caching**: Cache frequently accessed data\n- **Read Replicas**: Use read replicas for reporting queries\n- **Data Archiving**: Move old data to archive tables",
          "code": "-- Example: Query Analysis\n-- Before optimization (slow query)\nEXPLAIN ANALYZE\nSELECT * FROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.order_date > '2023-01-01'\nORDER BY o.total_amount DESC;\n\n-- After optimization (faster query)\n-- 1. Add covering index\nCREATE INDEX idx_orders_covering ON orders(customer_id, order_date, total_amount);\n\n-- 2. Optimized query with specific columns\nEXPLAIN ANALYZE\nSELECT o.id, o.order_date, o.total_amount, c.name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.order_date > '2023-01-01'\nORDER BY o.total_amount DESC\nLIMIT 100;\n\n-- Example: Subquery to JOIN conversion\n-- Before (subquery)\nSELECT * FROM customers\nWHERE id IN (\n    SELECT customer_id FROM orders \n    WHERE order_date > '2023-01-01'\n);\n\n-- After (JOIN)\nSELECT DISTINCT c.*\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nWHERE o.order_date > '2023-01-01';\n\n-- Example: Composite Index\nCREATE INDEX idx_orders_composite ON orders(customer_id, order_date, status);\n\n-- Example: Partial Index\nCREATE INDEX idx_active_orders ON orders(order_date, total_amount)\nWHERE status = 'active';"
        },
        {
          "name": "Database Scaling",
          "text": "# Database Scaling Strategies\n\n**Question:** How would you scale a database that's experiencing performance issues due to high read/write load?\n\n## **Answer:**\n\n### **Vertical Scaling (Scale Up)**\n- **Hardware Upgrade**: Increase CPU, RAM, and storage\n- **Database Tuning**: Optimize configuration parameters\n- **Memory Allocation**: Increase buffer pool and cache sizes\n- **Storage Optimization**: Use faster SSDs and RAID configurations\n\n### **Horizontal Scaling (Scale Out)**\n\n#### **1. Read Replicas**\n- **Master-Slave**: One master for writes, multiple slaves for reads\n- **Load Balancing**: Distribute read queries across replicas\n- **Replication Lag**: Monitor and minimize replication delay\n- **Failover**: Implement automatic failover mechanisms\n\n#### **2. Database Sharding**\n- **Horizontal Partitioning**: Split data across multiple databases\n- **Shard Key Strategy**: Choose appropriate sharding keys\n- **Cross-Shard Queries**: Handle queries spanning multiple shards\n- **Data Rebalancing**: Implement shard rebalancing strategies\n\n#### **3. Caching Layer**\n- **Redis/Memcached**: Cache frequently accessed data\n- **Application-Level Caching**: Cache query results in application\n- **CDN**: Use content delivery networks for static data\n- **Cache Invalidation**: Implement proper cache invalidation\n\n### **Microservices Database Pattern**\n- **Database per Service**: Each service owns its data\n- **Event Sourcing**: Store events instead of current state\n- **CQRS**: Separate read and write models\n- **Saga Pattern**: Handle distributed transactions\n\n### **Cloud Database Solutions**\n- **Managed Services**: Use cloud provider database services\n- **Auto-Scaling**: Leverage automatic scaling features\n- **Multi-Region**: Deploy across multiple regions\n- **Backup & Recovery**: Implement robust backup strategies",
          "code": "-- Example: Read Replica Setup (PostgreSQL)\n-- Master configuration\n-- postgresql.conf\nwal_level = replica\nmax_wal_senders = 3\nmax_replication_slots = 3\n\n-- Slave configuration\n-- recovery.conf\nstandby_mode = 'on'\nprimary_conninfo = 'host=master_host port=5432 user=replicator'\n\n-- Example: Database Sharding (MySQL)\n-- Shard 1: Users 1-1000000\nCREATE TABLE users_shard_1 (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n\n-- Shard 2: Users 1000001-2000000\nCREATE TABLE users_shard_2 (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n\n-- Sharding function\nDELIMITER //\nCREATE FUNCTION get_shard_id(user_id INT) RETURNS INT\nBEGIN\n    RETURN (user_id - 1) DIV 1000000 + 1;\nEND //\nDELIMITER ;\n\n-- Example: Redis Caching\n-- Cache user data\nSET user:123 '{\"name\":\"John\",\"email\":\"john@example.com\"}' EX 3600\n\n-- Get cached data\nGET user:123\n\n-- Example: Connection Pooling (Java)\n// HikariCP configuration\nHikariConfig config = new HikariConfig();\nconfig.setMaximumPoolSize(20);\nconfig.setMinimumIdle(5);\nconfig.setConnectionTimeout(30000);\nconfig.setIdleTimeout(600000);\nconfig.setMaxLifetime(1800000);"
        }
      ]
    },
    {
      "id": "microservices-communication-qa",
      "title": "🔗 Microservices Communication Q&A",
      "sections": [
        {
          "name": "Service Communication Failures",
          "text": "# Microservices Communication Failure Handling\n\n**Question:** What are the strategies you use to handle inter-service communication failures in mid-requests of microservices implementation?\n\n## **Answer:**\n\n### **1. Circuit Breaker Pattern**\n- **Purpose**: Prevent cascading failures by breaking the circuit when service is down\n- **States**: Closed (normal), Open (failing), Half-Open (testing)\n- **Implementation**: Hystrix, Resilience4j, or custom implementation\n- **Benefits**: Fast failure detection and recovery\n\n### **2. Retry Mechanisms**\n- **Exponential Backoff**: Gradually increase delay between retries\n- **Jitter**: Add randomness to prevent thundering herd\n- **Max Retries**: Limit number of retry attempts\n- **Retryable Errors**: Only retry on specific error types\n\n### **3. Timeout Configuration**\n- **Connection Timeout**: Time to establish connection\n- **Read Timeout**: Time to wait for response\n- **Circuit Breaker Timeout**: Time before trying failed service again\n- **Graceful Degradation**: Fallback to cached/default data\n\n### **4. Bulkhead Pattern**\n- **Resource Isolation**: Separate thread pools for different services\n- **Connection Pooling**: Dedicated connection pools per service\n- **Memory Isolation**: Prevent one service failure from affecting others\n- **CPU Isolation**: Use separate CPU cores for critical services\n\n### **5. Fallback Strategies**\n- **Default Values**: Return sensible defaults when service fails\n- **Cached Data**: Use stale but available cached data\n- **Alternative Services**: Route to backup services\n- **Graceful Degradation**: Reduce functionality but maintain core features\n\n### **6. Monitoring & Observability**\n- **Health Checks**: Regular service health monitoring\n- **Metrics**: Track success/failure rates, response times\n- **Distributed Tracing**: Track requests across services\n- **Alerting**: Immediate notification of service failures\n\n### **7. Event-Driven Architecture**\n- **Asynchronous Communication**: Use message queues/event streams\n- **Event Sourcing**: Store events instead of current state\n- **Saga Pattern**: Handle distributed transactions\n- **Compensating Actions**: Rollback operations across services",
          "code": "// Example: Circuit Breaker Implementation (Java)\n@Component\npublic class UserServiceClient {\n    \n    private final CircuitBreaker circuitBreaker;\n    \n    public UserServiceClient() {\n        this.circuitBreaker = CircuitBreaker.ofDefaults(\"userService\")\n            .toBuilder()\n            .failureRateThreshold(50)\n            .waitDurationInOpenState(Duration.ofSeconds(30))\n            .slidingWindowSize(10)\n            .build();\n    }\n    \n    public User getUser(String userId) {\n        return circuitBreaker.executeSupplier(() -> {\n            try {\n                return userService.getUser(userId);\n            } catch (Exception e) {\n                return getDefaultUser(userId); // Fallback\n            }\n        });\n    }\n    \n    private User getDefaultUser(String userId) {\n        return User.builder()\n            .id(userId)\n            .name(\"Unknown User\")\n            .email(\"unknown@example.com\")\n            .build();\n    }\n}\n\n// Example: Retry with Exponential Backoff\n@Retryable(\n    value = {ConnectException.class, SocketTimeoutException.class},\n    maxAttempts = 3,\n    backoff = @Backoff(delay = 1000, multiplier = 2)\n)\npublic ResponseEntity<String> callExternalService(String data) {\n    return restTemplate.postForEntity(\"/external-service\", data, String.class);\n}\n\n// Example: Timeout Configuration\n@Configuration\npublic class RestTemplateConfig {\n    \n    @Bean\n    public RestTemplate restTemplate() {\n        HttpComponentsClientHttpRequestFactory factory = \n            new HttpComponentsClientHttpRequestFactory();\n        factory.setConnectTimeout(5000); // 5 seconds\n        factory.setReadTimeout(10000);    // 10 seconds\n        \n        return new RestTemplate(factory);\n    }\n}\n\n// Example: Bulkhead Pattern\n@Service\npublic class OrderService {\n    \n    @Async(\"orderTaskExecutor\")\n    public CompletableFuture<Order> processOrder(OrderRequest request) {\n        // Process order in dedicated thread pool\n        return CompletableFuture.completedFuture(\n            orderProcessor.process(request)\n        );\n    }\n}\n\n@Configuration\n@EnableAsync\npublic class AsyncConfig {\n    \n    @Bean(\"orderTaskExecutor\")\n    public Executor orderTaskExecutor() {\n        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n        executor.setCorePoolSize(5);\n        executor.setMaxPoolSize(10);\n        executor.setQueueCapacity(100);\n        executor.setThreadNamePrefix(\"OrderService-\");\n        executor.initialize();\n        return executor;\n    }\n}"
        },
        {
          "name": "Service Discovery",
          "text": "# Service Discovery & Load Balancing\n\n**Question:** How do you handle service discovery and load balancing in a microservices architecture?\n\n## **Answer:**\n\n### **Service Discovery Patterns**\n\n#### **1. Client-Side Discovery**\n- **Service Registry**: Central registry (Eureka, Consul, etcd)\n- **Client Responsibility**: Client queries registry and selects service instance\n- **Load Balancing**: Client implements load balancing logic\n- **Benefits**: Simple implementation, no additional network hop\n\n#### **2. Server-Side Discovery**\n- **Load Balancer**: Central load balancer (AWS ALB, NGINX, HAProxy)\n- **Service Registry**: Load balancer queries registry\n- **Client Simplicity**: Client only knows load balancer endpoint\n- **Benefits**: Centralized load balancing, client simplicity\n\n### **Service Registry Features**\n- **Health Checks**: Regular health monitoring of services\n- **Service Registration**: Automatic service registration/deregistration\n- **Metadata**: Store service metadata (version, tags, etc.)\n- **High Availability**: Multiple registry instances for fault tolerance\n\n### **Load Balancing Strategies**\n- **Round Robin**: Distribute requests evenly across instances\n- **Weighted Round Robin**: Assign different weights to instances\n- **Least Connections**: Route to instance with fewest active connections\n- **IP Hash**: Route based on client IP hash\n- **Geographic**: Route based on geographic location\n\n### **Service Mesh**\n- **Istio**: Service mesh for Kubernetes\n- **Envoy Proxy**: Sidecar proxy for service communication\n- **Traffic Management**: Advanced routing and load balancing\n- **Security**: mTLS, authentication, authorization\n- **Observability**: Metrics, logging, tracing\n\n### **Implementation Considerations**\n- **Service Registration**: Automatic registration on startup\n- **Health Checks**: Regular health monitoring\n- **Graceful Shutdown**: Proper deregistration on shutdown\n- **Configuration**: Externalized configuration management\n- **Security**: Secure service-to-service communication",
          "code": "// Example: Eureka Service Registration (Spring Boot)\n@SpringBootApplication\n@EnableEurekaClient\npublic class UserServiceApplication {\n    \n    public static void main(String[] args) {\n        SpringApplication.run(UserServiceApplication.class, args);\n    }\n}\n\n// application.yml\nspring:\n  application:\n    name: user-service\n  cloud:\n    consul:\n      host: localhost\n      port: 8500\n      discovery:\n        service-name: ${spring.application.name}\n        health-check-interval: 15s\n        health-check-critical-timeout: 30s\n\n// Example: Service Discovery Client\n@Service\npublic class OrderServiceClient {\n    \n    @Autowired\n    private DiscoveryClient discoveryClient;\n    \n    @Autowired\n    private RestTemplate restTemplate;\n    \n    public Order createOrder(OrderRequest request) {\n        List<ServiceInstance> instances = \n            discoveryClient.getInstances(\"order-service\");\n        \n        if (instances.isEmpty()) {\n            throw new ServiceUnavailableException(\"Order service not available\");\n        }\n        \n        ServiceInstance instance = instances.get(0);\n        String url = String.format(\"http://%s:%d/orders\", \n            instance.getHost(), instance.getPort());\n        \n        return restTemplate.postForObject(url, request, Order.class);\n    }\n}\n\n// Example: Load Balancer Configuration (NGINX)\nupstream user_service {\n    server user-service-1:8080 weight=3;\n    server user-service-2:8080 weight=2;\n    server user-service-3:8080 weight=1;\n    \n    # Health checks\n    health_check interval=10s fails=3 passes=2;\n}\n\nserver {\n    listen 80;\n    \n    location /api/users/ {\n        proxy_pass http://user_service;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n\n// Example: Istio Service Mesh Configuration\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: user-service\nspec:\n  http:\n  - match:\n    - headers:\n        version:\n          exact: v2\n    route:\n    - destination:\n        host: user-service\n        subset: v2\n  - route:\n    - destination:\n        host: user-service\n        subset: v1\n      weight: 90\n    - destination:\n        host: user-service\n        subset: v2\n      weight: 10"
        },
        {
          "name": "Distributed Transactions",
          "text": "# Distributed Transactions in Microservices\n\n**Question:** How do you handle distributed transactions across multiple microservices?\n\n## **Answer:**\n\n### **Challenges with Distributed Transactions**\n- **ACID Properties**: Difficult to maintain across services\n- **Network Failures**: Network partitions and timeouts\n- **Performance**: Two-phase commit is slow and blocking\n- **Availability**: Reduces system availability\n- **Complexity**: Complex failure handling and recovery\n\n### **Saga Pattern**\n\n#### **1. Choreography-Based Saga**\n- **Event-Driven**: Services communicate via events\n- **Decentralized**: No central coordinator\n- **Eventual Consistency**: Eventually consistent state\n- **Benefits**: Loose coupling, high availability\n- **Drawbacks**: Complex debugging, difficult to track\n\n#### **2. Orchestration-Based Saga**\n- **Central Coordinator**: Central service orchestrates transactions\n- **Command Pattern**: Coordinator sends commands to services\n- **State Management**: Coordinator maintains transaction state\n- **Benefits**: Easy to understand, centralized control\n- **Drawbacks**: Single point of failure, tight coupling\n\n### **Compensating Actions**\n- **Compensating Transactions**: Actions that undo previous operations\n- **Idempotent Operations**: Safe to retry operations\n- **Event Sourcing**: Store events instead of current state\n- **CQRS**: Separate read and write models\n\n### **Event Sourcing**\n- **Event Store**: Store all events that happened\n- **Replay**: Replay events to reconstruct current state\n- **Audit Trail**: Complete history of all changes\n- **Temporal Queries**: Query state at any point in time\n\n### **Implementation Strategies**\n- **Idempotency**: Ensure operations can be safely retried\n- **Eventual Consistency**: Accept temporary inconsistency\n- **Compensation**: Implement compensating actions\n- **Monitoring**: Track transaction state and failures\n- **Testing**: Test failure scenarios and recovery\n\n### **Best Practices**\n- **Design for Failure**: Assume services will fail\n- **Idempotent Operations**: Make operations safe to retry\n- **Timeout Handling**: Implement proper timeouts\n- **Monitoring**: Track transaction success/failure rates\n- **Documentation**: Document transaction boundaries and compensation logic",
          "code": "// Example: Choreography-Based Saga\n// Order Service\n@Service\npublic class OrderService {\n    \n    @Autowired\n    private ApplicationEventPublisher eventPublisher;\n    \n    public void createOrder(OrderRequest request) {\n        Order order = new Order(request);\n        orderRepository.save(order);\n        \n        // Publish event for payment service\n        eventPublisher.publishEvent(new OrderCreatedEvent(order.getId(), request.getAmount()));\n    }\n    \n    @EventListener\n    public void handlePaymentFailed(PaymentFailedEvent event) {\n        Order order = orderRepository.findById(event.getOrderId());\n        order.cancel();\n        orderRepository.save(order);\n        \n        // Publish compensation event\n        eventPublisher.publishEvent(new OrderCancelledEvent(order.getId()));\n    }\n}\n\n// Payment Service\n@Service\npublic class PaymentService {\n    \n    @Autowired\n    private ApplicationEventPublisher eventPublisher;\n    \n    @EventListener\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        try {\n            Payment payment = processPayment(event.getAmount());\n            eventPublisher.publishEvent(new PaymentSucceededEvent(event.getOrderId(), payment.getId()));\n        } catch (PaymentException e) {\n            eventPublisher.publishEvent(new PaymentFailedEvent(event.getOrderId(), e.getMessage()));\n        }\n    }\n}\n\n// Example: Orchestration-Based Saga\n@Service\npublic class OrderSagaOrchestrator {\n    \n    @Autowired\n    private OrderServiceClient orderServiceClient;\n    \n    @Autowired\n    private PaymentServiceClient paymentServiceClient;\n    \n    @Autowired\n    private InventoryServiceClient inventoryServiceClient;\n    \n    public void processOrder(OrderRequest request) {\n        SagaContext context = new SagaContext();\n        \n        try {\n            // Step 1: Create Order\n            Order order = orderServiceClient.createOrder(request);\n            context.setOrderId(order.getId());\n            \n            // Step 2: Process Payment\n            Payment payment = paymentServiceClient.processPayment(order.getAmount());\n            context.setPaymentId(payment.getId());\n            \n            // Step 3: Reserve Inventory\n            inventoryServiceClient.reserveInventory(request.getItems());\n            \n            // All steps succeeded\n            context.setStatus(SagaStatus.COMPLETED);\n            \n        } catch (Exception e) {\n            // Compensate for completed steps\n            compensate(context);\n            context.setStatus(SagaStatus.FAILED);\n        }\n    }\n    \n    private void compensate(SagaContext context) {\n        // Compensate in reverse order\n        if (context.getPaymentId() != null) {\n            paymentServiceClient.refundPayment(context.getPaymentId());\n        }\n        \n        if (context.getOrderId() != null) {\n            orderServiceClient.cancelOrder(context.getOrderId());\n        }\n    }\n}\n\n// Example: Event Sourcing\n@Entity\npublic class OrderAggregate {\n    \n    @Id\n    private String id;\n    private List<DomainEvent> events = new ArrayList<>();\n    \n    public void createOrder(OrderRequest request) {\n        OrderCreatedEvent event = new OrderCreatedEvent(id, request);\n        events.add(event);\n        apply(event);\n    }\n    \n    public void cancelOrder() {\n        OrderCancelledEvent event = new OrderCancelledEvent(id);\n        events.add(event);\n        apply(event);\n    }\n    \n    private void apply(DomainEvent event) {\n        // Apply event to aggregate state\n        if (event instanceof OrderCreatedEvent) {\n            // Update aggregate state\n        } else if (event instanceof OrderCancelledEvent) {\n            // Update aggregate state\n        }\n    }\n    \n    public List<DomainEvent> getUncommittedEvents() {\n        return events;\n    }\n    \n    public void markEventsAsCommitted() {\n        events.clear();\n    }\n}"
        }
      ]
    },
    {
      "id": "system-design-qa",
      "title": "🏗️ System Design Q&A",
      "sections": [
        {
          "name": "Scalability Patterns",
          "text": "# System Scalability Patterns\n\n**Question:** How would you design a system to handle 1 million concurrent users?\n\n## **Answer:**\n\n### **1. Load Balancing**\n- **Multiple Load Balancers**: Use multiple load balancers for redundancy\n- **Geographic Distribution**: Deploy across multiple regions\n- **DNS Load Balancing**: Use DNS to distribute traffic\n- **Application Load Balancers**: Layer 7 load balancing for intelligent routing\n\n### **2. Horizontal Scaling**\n- **Stateless Services**: Design stateless services for easy scaling\n- **Auto-Scaling**: Implement automatic scaling based on metrics\n- **Container Orchestration**: Use Kubernetes for container management\n- **Microservices**: Break monolith into smaller services\n\n### **3. Caching Strategy**\n- **Multi-Level Caching**: Browser, CDN, Application, Database\n- **Cache Invalidation**: Implement proper cache invalidation\n- **Cache Warming**: Pre-populate cache with frequently accessed data\n- **Distributed Caching**: Use Redis or Memcached for shared cache\n\n### **4. Database Optimization**\n- **Read Replicas**: Use read replicas for read-heavy workloads\n- **Database Sharding**: Partition data across multiple databases\n- **Connection Pooling**: Optimize database connections\n- **Query Optimization**: Optimize slow queries and add indexes\n\n### **5. Asynchronous Processing**\n- **Message Queues**: Use message queues for asynchronous processing\n- **Event-Driven Architecture**: Implement event-driven patterns\n- **Background Jobs**: Move heavy processing to background\n- **WebSockets**: Use WebSockets for real-time communication\n\n### **6. CDN and Static Assets**\n- **Content Delivery Network**: Use CDN for static assets\n- **Asset Optimization**: Compress and optimize static files\n- **Edge Caching**: Cache content at edge locations\n- **Image Optimization**: Use appropriate image formats and sizes\n\n### **7. Monitoring and Observability**\n- **Application Metrics**: Track key performance indicators\n- **Distributed Tracing**: Track requests across services\n- **Log Aggregation**: Centralize and analyze logs\n- **Alerting**: Set up alerts for critical metrics\n\n### **8. Security Considerations**\n- **Rate Limiting**: Implement rate limiting to prevent abuse\n- **DDoS Protection**: Use DDoS protection services\n- **Authentication**: Implement secure authentication\n- **Data Encryption**: Encrypt data in transit and at rest",
          "code": "// Example: Auto-Scaling Configuration (Kubernetes)\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: user-service-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: user-service\n  minReplicas: 3\n  maxReplicas: 100\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n\n// Example: Load Balancer Configuration (NGINX)\nupstream backend {\n    least_conn;\n    server app1.example.com:8080 max_fails=3 fail_timeout=30s;\n    server app2.example.com:8080 max_fails=3 fail_timeout=30s;\n    server app3.example.com:8080 max_fails=3 fail_timeout=30s;\n}\n\nserver {\n    listen 80;\n    \n    location / {\n        proxy_pass http://backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        \n        # Health checks\n        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;\n    }\n}\n\n// Example: Caching Strategy (Redis)\n@Service\npublic class UserService {\n    \n    @Autowired\n    private RedisTemplate<String, Object> redisTemplate;\n    \n    @Cacheable(value = \"users\", key = \"#userId\")\n    public User getUser(String userId) {\n        return userRepository.findById(userId);\n    }\n    \n    @CacheEvict(value = \"users\", key = \"#user.id\")\n    public User updateUser(User user) {\n        User updatedUser = userRepository.save(user);\n        \n        // Publish cache invalidation event\n        redisTemplate.convertAndSend(\"cache-invalidation\", \n            new CacheInvalidationEvent(\"users\", user.getId()));\n        \n        return updatedUser;\n    }\n}\n\n// Example: Message Queue (RabbitMQ)\n@Component\npublic class OrderProcessor {\n    \n    @RabbitListener(queues = \"order.queue\")\n    public void processOrder(OrderMessage message) {\n        try {\n            // Process order asynchronously\n            Order order = orderService.createOrder(message.getOrderRequest());\n            \n            // Send confirmation email\n            emailService.sendOrderConfirmation(order);\n            \n        } catch (Exception e) {\n            // Send to dead letter queue\n            rabbitTemplate.send(\"order.dlq\", message);\n        }\n    }\n}"
        },
        {
          "name": "Data Consistency",
          "text": "# Data Consistency in Distributed Systems\n\n**Question:** How do you ensure data consistency across multiple services in a distributed system?\n\n## **Answer:**\n\n### **Consistency Models**\n\n#### **1. Strong Consistency (ACID)**\n- **Immediate Consistency**: All nodes see the same data immediately\n- **Use Cases**: Financial transactions, critical business data\n- **Trade-offs**: Higher latency, reduced availability\n- **Implementation**: Two-phase commit, distributed locks\n\n#### **2. Eventual Consistency**\n- **Eventually Consistent**: Data becomes consistent over time\n- **Use Cases**: Social media, content management, analytics\n- **Trade-offs**: Temporary inconsistency, eventual consistency\n- **Implementation**: Event sourcing, CQRS, conflict resolution\n\n#### **3. Weak Consistency**\n- **No Guarantees**: No consistency guarantees\n- **Use Cases**: Real-time data, sensor data, logs\n- **Trade-offs**: Fast performance, no consistency guarantees\n- **Implementation**: Best-effort updates, eventual consistency\n\n### **Consistency Patterns**\n\n#### **1. Event Sourcing**\n- **Event Store**: Store all events that happened\n- **Replay**: Replay events to reconstruct current state\n- **Audit Trail**: Complete history of all changes\n- **Benefits**: Complete audit trail, temporal queries\n\n#### **2. CQRS (Command Query Responsibility Segregation)**\n- **Separate Models**: Different models for reads and writes\n- **Event-Driven**: Commands generate events, queries read from views\n- **Scalability**: Scale reads and writes independently\n- **Benefits**: Better performance, simpler models\n\n#### **3. Saga Pattern**\n- **Distributed Transactions**: Handle transactions across services\n- **Compensating Actions**: Actions that undo previous operations\n- **Eventual Consistency**: Accept temporary inconsistency\n- **Benefits**: Better availability, eventual consistency\n\n### **Conflict Resolution Strategies**\n- **Last Writer Wins**: Use timestamp to resolve conflicts\n- **Vector Clocks**: Use vector clocks for causality\n- **CRDTs**: Use Conflict-free Replicated Data Types\n- **Application Logic**: Use business logic to resolve conflicts\n\n### **Implementation Considerations**\n- **Idempotency**: Ensure operations can be safely retried\n- **Ordering**: Ensure proper ordering of operations\n- **Monitoring**: Track consistency metrics\n- **Testing**: Test consistency scenarios\n- **Documentation**: Document consistency guarantees",
          "code": "// Example: Event Sourcing Implementation\n@Entity\npublic class OrderAggregate {\n    \n    @Id\n    private String id;\n    private OrderStatus status;\n    private List<DomainEvent> events = new ArrayList<>();\n    \n    public void createOrder(OrderRequest request) {\n        OrderCreatedEvent event = new OrderCreatedEvent(id, request);\n        events.add(event);\n        apply(event);\n    }\n    \n    public void cancelOrder() {\n        OrderCancelledEvent event = new OrderCancelledEvent(id);\n        events.add(event);\n        apply(event);\n    }\n    \n    private void apply(DomainEvent event) {\n        if (event instanceof OrderCreatedEvent) {\n            this.status = OrderStatus.CREATED;\n        } else if (event instanceof OrderCancelledEvent) {\n            this.status = OrderStatus.CANCELLED;\n        }\n    }\n    \n    public List<DomainEvent> getUncommittedEvents() {\n        return events;\n    }\n}\n\n// Example: CQRS Implementation\n// Command Side\n@Service\npublic class OrderCommandService {\n    \n    @Autowired\n    private OrderRepository orderRepository;\n    \n    @Autowired\n    private EventPublisher eventPublisher;\n    \n    public void createOrder(OrderRequest request) {\n        Order order = new Order(request);\n        orderRepository.save(order);\n        \n        // Publish event for query side\n        eventPublisher.publishEvent(new OrderCreatedEvent(order));\n    }\n}\n\n// Query Side\n@Service\npublic class OrderQueryService {\n    \n    @Autowired\n    private OrderViewRepository orderViewRepository;\n    \n    @EventListener\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        OrderView view = new OrderView(event.getOrder());\n        orderViewRepository.save(view);\n    }\n    \n    public List<OrderView> getOrdersByCustomer(String customerId) {\n        return orderViewRepository.findByCustomerId(customerId);\n    }\n}\n\n// Example: Conflict Resolution with Vector Clocks\npublic class VectorClock {\n    private Map<String, Long> clock = new HashMap<>();\n    \n    public void increment(String nodeId) {\n        clock.put(nodeId, clock.getOrDefault(nodeId, 0L) + 1);\n    }\n    \n    public boolean happensBefore(VectorClock other) {\n        boolean strictlyLess = false;\n        \n        for (String nodeId : clock.keySet()) {\n            long thisTime = clock.get(nodeId);\n            long otherTime = other.clock.getOrDefault(nodeId, 0L);\n            \n            if (thisTime > otherTime) {\n                return false;\n            }\n            if (thisTime < otherTime) {\n                strictlyLess = true;\n            }\n        }\n        \n        return strictlyLess;\n    }\n}\n\n// Example: CRDT Implementation (G-Counter)\npublic class GCounter {\n    private Map<String, Long> counters = new HashMap<>();\n    \n    public void increment(String nodeId) {\n        counters.put(nodeId, counters.getOrDefault(nodeId, 0L) + 1);\n    }\n    \n    public long getValue() {\n        return counters.values().stream().mapToLong(Long::longValue).sum();\n    }\n    \n    public void merge(GCounter other) {\n        for (Map.Entry<String, Long> entry : other.counters.entrySet()) {\n            String nodeId = entry.getKey();\n            long otherValue = entry.getValue();\n            long thisValue = counters.getOrDefault(nodeId, 0L);\n            \n            counters.put(nodeId, Math.max(thisValue, otherValue));\n        }\n    }\n}"
        }
      ]
    },
    {
      "id": "kubernetes-debugging-qa",
      "title": "☸️ Kubernetes Deep Dive Debugging Q&A",
      "sections": [
        {
          "name": "Pod Stuck in CrashLoopBackOff",
          "text": "# Pod stuck in CrashLoopBackOff - Advanced Debugging\n\n**Question:** Pod stuck in CrashLoopBackOff, no logs, no errors. How do you debug beyond kubectl logs and describe?\n\n## **Answer:**\n\nWhen standard debugging yields no results, employ these advanced techniques:\n\n### **1. Ephemeral Containers for Live Debugging**\nUse ephemeral containers to debug running (or crashing) pods without modifying the original container. This allows you to attach debugging tools to a problematic pod:\n```bash\nkubectl debug <pod-name> -it --image=busybox --target=<container-name>\n```\n\n### **2. Check Previous Container Logs**\nUse the `-p` flag to view logs from the crashed container instance:\n```bash\nkubectl logs <pod-name> -p\n```\n\n### **3. Investigate Image and Resource Issues**\nExamine pod events for image pull failures, OCI runtime errors, or resource allocation problems. Check if the container has sufficient CPU/memory requests and limits.\n\n### **4. Analyze Init Containers**\nFailed init containers can prevent the main container from starting. Check their status and logs separately.\n\n### **5. Examine ConfigMaps and Secrets**\nVerify all referenced ConfigMaps and Secrets exist and contain valid data. Missing or malformed configuration can cause immediate crashes.\n\n### **6. Check Liveness/Readiness Probes**\nMisconfigured health probes can cause containers to restart prematurely. Review probe configurations for appropriate timeout, period, and failure thresholds.",
          "code": "# Example: Check pod events for issues\nkubectl describe pod <pod-name>\n\n# Check init containers\nkubectl logs <pod-name> -c <init-container-name>\n\n# Check ConfigMaps and Secrets\nkubectl get configmap <configmap-name>\nkubectl get secret <secret-name>\n\n# Verify resource requests/limits\nkubectl get pod <pod-name> -o jsonpath='{.spec.containers[*].resources}'\n\n# Example: Proper health probes\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  failureThreshold: 3\n  timeoutSeconds: 5"
        },
        {
          "name": "StatefulSet PVC Reattachment",
          "text": "# StatefulSet pod won't reattach PVC after node crash\n\n**Question:** A StatefulSet pod won't reattach its PVC after a node crash. How do you recover without recreating storage?\n\n## **Answer:**\n\nThis is a common issue where VolumeAttachments become stuck:\n\n### **1. Identify Stuck VolumeAttachments**\n```bash\nkubectl get volumeattachment\nkubectl get pv\n```\n\n### **2. Remove Stale VolumeAttachments**\nDelete the stuck VolumeAttachment resource (this doesn't delete the volume data):\n```bash\nkubectl delete volumeattachment <attachment-name>\n```\n\n### **3. Force Deletion with Finalizer Removal**\nIf the VolumeAttachment won't delete, remove its finalizers:\n```bash\nkubectl patch volumeattachment <attachment-name> -p '{\"metadata\":{\"finalizers\":null}}'\n```\n\n### **4. Manually Delete and Recreate the Pod**\nForce delete the pod to trigger rescheduling:\n```bash\nkubectl delete pod <pod-name> --force --grace-period=0\n```\n\n### **5. Verify PVC Policy**\nStatefulSets retain PVCs by default. The `persistentVolumeClaimRetentionPolicy` determines whether PVCs are deleted when pods are removed. Ensure the PVC still exists and hasn't been accidentally deleted.\n\n### **6. Check Node Status**\nVerify the node status and ensure kubelet is running. If the node is truly dead, you may need to force delete the node object to release volume attachments.",
          "code": "# Check volume attachments and PV status\nkubectl get volumeattachment\nkubectl get pv\nkubectl get pvc\n\n# Check node status\nkubectl get nodes\nkubectl describe node <node-name>\n\n# Force delete stuck pod\nkubectl delete pod <pod-name> --force --grace-period=0\n\n# Check StatefulSet PVC retention policy\nkubectl get statefulset <statefulset-name> -o yaml | grep -A 5 persistentVolumeClaimRetentionPolicy"
        },
        {
          "name": "Cluster Autoscaler Debugging",
          "text": "# Pods are Pending, Cluster Autoscaler won't scale up\n\n**Question:** Pods are Pending, Cluster Autoscaler won't scale up. Walk me through your top 3 debugging steps.\n\n## **Answer:**\n\nThe Cluster Autoscaler only scales when it detects unschedulable pods.\n\n### **Step 1: Verify Unschedulable Pods and Scheduler Events**\nCheck for pods with `PodUnschedulable` status and examine their events:\n```bash\nkubectl get pods --field-selector=status.phase=Pending\nkubectl describe pod <pending-pod-name>\nkubectl logs -n kube-system -l component=kube-scheduler\n```\nCommon causes: insufficient CPU/memory, node selectors/affinity rules, taints/tolerations mismatches, or resource quota limits.\n\n### **Step 2: Check Autoscaler Logs and Configuration**\nExamine Cluster Autoscaler logs for scale-up decisions:\n```bash\nkubectl logs -n kube-system -l app=cluster-autoscaler\nkubectl describe configmap cluster-autoscaler-status -n kube-system\n```\nVerify the autoscaler sees the pending pod, ensure pods match node templates, and check that min/max node counts aren't reached.\n\n### **Step 3: Validate Pod Resource Requests vs. Node Capacity**\nCompare pod resource requests with available node types:\n```bash\nkubectl describe nodes\nkubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory\n```\nCheck if pod requests exceed any single node's capacity. Verify node pools are configured correctly and cloud provider quotas are within limits.",
          "code": "# Check pending pods with details\nkubectl get pods --field-selector=status.phase=Pending -o wide\nkubectl describe pod <pending-pod-name>\n\n# Check autoscaler logs\nkubectl logs -n kube-system -l app=cluster-autoscaler --tail=100\n\n# Check node capacity and resources\nkubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory,STORAGE:.status.capacity.ephemeral-storage\nkubectl describe nodes\n\n# Check if pod requests exceed node capacity\nkubectl get pod <pod-name> -o jsonpath='{.spec.containers[*].resources.requests}'"
        },
        {
          "name": "NetworkPolicy Design",
          "text": "# NetworkPolicy blocks cross-namespace traffic\n\n**Question:** NetworkPolicy blocks cross-namespace traffic. How do you design least-privilege rules and test them safely?\n\n## **Answer:**\n\n### **Start with Default Deny**\nApply a default deny-all policy in each namespace as the baseline:\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-cross-namespace\n  namespace: <target-namespace>\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector: {}  # Allow only from same namespace\n```\n\n### **Design Least-Privilege Rules**\n- Use specific pod selectors and namespace selectors with consistent labels\n- Allow only required ports and protocols\n- Document each policy's purpose for maintainability\n- Start narrow and expand only as needed\n\n### **Example: Allow specific cross-namespace access**\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-specific-namespace\n  namespace: backend\nspec:\n  podSelector:\n    matchLabels:\n      app: database\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          environment: production\n      podSelector:\n        matchLabels:\n          app: api\n    ports:\n    - protocol: TCP\n      port: 3306\n```\n\n### **Test Safely**\n- Deploy policies in `Audit` mode first (if using tools like Calico Enterprise)\n- Use test pods to verify connectivity before enforcing policies\n- Monitor denied connections in CNI logs\n- Create temporary test namespaces with similar configurations\n\n```bash\n# Test connectivity from one pod to another\nkubectl run test-pod --image=busybox -it --rm -- wget -O- http://service.namespace.svc.cluster.local\n```",
          "code": "# Example: Default deny all policy\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\n# Example: Allow intra-namespace communication\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-same-namespace\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: frontend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          environment: production\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - protocol: TCP\n      port: 5432\n\n# Test network connectivity\nkubectl run debug --image=busybox -it --rm -- wget -O- http://service.namespace.svc:8080\n\n# Check NetworkPolicy status\nkubectl get networkpolicies -n <namespace>\nkubectl describe networkpolicy <policy-name> -n <namespace>"
        },
        {
          "name": "VPN Architecture in K8s",
          "text": "# Service must connect to external DB via VPN\n\n**Question:** Service must connect to an external DB via VPN inside the cluster. How do you architect it for HA + security?\n\n## **Answer:**\n\n### **VPN Client Deployment Architecture**\n- Deploy VPN client (WireGuard, OpenVPN, or IPSec) as a dedicated deployment or sidecar\n- Use route-based VPN to route only DB subnet traffic through VPN tunnel\n- Keep cluster-internal traffic on normal network paths\n\n### **High Availability Design**\n- Run multiple VPN client replicas (minimum 3) across different nodes\n- Implement Pod Disruption Budgets to maintain minimum availability\n- Use node affinity rules to spread VPN pods across availability zones\n- Configure readiness and liveness probes on VPN tunnel health\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: vpn-gateway\nspec:\n  type: ClusterIP\n  selector:\n    app: vpn-client\n  ports:\n  - protocol: TCP\n    port: 5432  # Database port\n```\n\n### **Security Hardening**\n- Store VPN credentials in Kubernetes Secrets, never in ConfigMaps or environment variables\n- Use NetworkPolicies to restrict which pods can access the VPN gateway service\n- Enforce mTLS for database connections even over VPN\n- Apply pod security standards to VPN client pods\n- Use RBAC to limit who can modify VPN configurations\n\n### **Connection Routing**\n- Create a Kubernetes Service that abstracts VPN gateway pods\n- Configure applications to connect to the Service, not directly to external DB\n- Use DNS-based service discovery or environment variables for connection strings\n- Implement connection pooling at the application level\n\n### **Monitoring and Failover**\n- Monitor VPN tunnel health with liveness probes\n- Track connection metrics to detect failures\n- Use Prometheus/Grafana for real-time visibility\n- Set up alerts for tunnel downtime or high latency",
          "code": "# Example: VPN Deployment with HA\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpn-gateway\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: vpn-client\n  template:\n    metadata:\n      labels:\n        app: vpn-client\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - vpn-client\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: vpn-client\n        image: vpn-client:latest\n        env:\n        - name: VPN_CONFIG\n          valueFrom:\n            secretKeyRef:\n              name: vpn-credentials\n              key: config\n        livenessProbe:\n          exec:\n            command:\n            - ping\n            - -c\n            - 1\n            - database.example.com\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - ping\n            - -c\n            - 1\n            - database.example.com\n          initialDelaySeconds: 10\n          periodSeconds: 5\n---\napiVersion: v1\nkind: PodDisruptionBudget\nmetadata:\n  name: vpn-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: vpn-client"
        },
        {
          "name": "Multi-tenant EKS Isolation",
          "text": "# Running a multi-tenant EKS cluster\n\n**Question:** Running a multi-tenant EKS cluster. How do you isolate workloads with RBAC, quotas, and network segmentation?\n\n## **Answer:**\n\nMulti-tenancy requires layered isolation:\n\n### **RBAC Isolation**\n- Create IAM roles mapped to Kubernetes RBAC groups via AWS IAM Authenticator\n- Define namespace-scoped Roles (not ClusterRoles) for tenant-specific permissions\n- Use separate ServiceAccounts for each tenant's workloads\n- Follow principle of least privilege—grant only necessary API access\n- Regularly audit role bindings and permissions\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: tenant-a\n  name: tenant-developer\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n```\n\n### **Resource Quotas and Limits**\n- Apply ResourceQuotas at the namespace level to limit CPU, memory, and object counts\n- Use LimitRanges to set default requests/limits and prevent resource hogging\n- Prevent one tenant from consuming all cluster resources\n\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: tenant-a-quota\n  namespace: tenant-a\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"20Gi\"\n    limits.cpu: \"20\"\n    limits.memory: \"40Gi\"\n    pods: \"50\"\n```\n\n### **Network Segmentation**\n- Deploy NetworkPolicies to isolate tenant traffic\n- Block cross-namespace communication by default\n- Allow only explicitly required inter-namespace traffic\n- Segment different environments (dev, staging, prod) using namespace labels\n\n### **Additional Isolation Layers**\n- Use dedicated node pools per tenant for complete compute isolation (hard multi-tenancy)\n- Apply Pod Security Standards to enforce security baselines\n- Use separate ingress controllers or configure ingress class per tenant\n- Implement separate monitoring and logging per tenant",
          "code": "# Example: Tenant namespace with isolation\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: tenant-a\n  labels:\n    tenant: tenant-a\n    environment: production\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: tenant-a-quota\n  namespace: tenant-a\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"20Gi\"\n    limits.cpu: \"20\"\n    limits.memory: \"40Gi\"\n    pods: \"50\"\n    persistentvolumeclaims: \"10\"\n---\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: tenant-a-limits\n  namespace: tenant-a\nspec:\n  limits:\n  - default:\n      cpu: \"1\"\n      memory: \"2Gi\"\n    defaultRequest:\n      cpu: \"100m\"\n      memory: \"256Mi\"\n    type: Container\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: tenant-isolation\n  namespace: tenant-a\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          tenant: tenant-a"
        },
        {
          "name": "Kubelet Restart Debugging",
          "text": "# Kubelet keeps restarting on one node\n\n**Question:** Kubelet keeps restarting on one node. Where do you look first – systemd, container runtime, or cgroups?\n\n## **Answer:**\n\nStart with **systemd** since kubelet runs as a systemd service.\n\n### **Check Systemd Logs First**\n```bash\njournalctl -xe --unit kubelet\nsystemctl status kubelet\n```\nLook for startup failures, crash patterns, or configuration errors. Kubelet failures often stem from misconfigurations or dependency issues.\n\n### **Verify Container Runtime Configuration**\nCheck if container runtime (containerd/Docker) is running and healthy:\n```bash\nsystemctl status containerd  # or docker\ndocker info  # or crictl info\n```\nVerify kubelet is configured to use the correct runtime socket and ensure runtime and kubelet are using compatible versions.\n\n### **Check cgroup Driver Mismatch (Critical)**\nThis is a **critical and common issue**. Kubelet and container runtime must use the same cgroup driver (`systemd` or `cgroupfs`). Mismatch causes instability under resource pressure.\n\n```bash\n# Check container runtime cgroup driver\ndocker info | grep -i cgroup\n# or for containerd\ncat /etc/containerd/config.toml | grep SystemdCgroup\n\n# Check kubelet config\ncat /var/lib/kubelet/config.yaml | grep cgroupDriver\n```\n\n**Fix cgroup mismatch**:\nSet in `/var/lib/kubelet/config.yaml`:\n```yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd\n```\n\nThen restart services:\n```bash\nsystemctl daemon-reload\nsystemctl restart containerd\nsystemctl restart kubelet\n```\n\n### **Additional Checks**\n- Verify swap is disabled (`swapoff -a`)\n- Check node resource exhaustion (disk space, memory, CPU)\n- Review kubelet configuration for errors\n- Examine kernel logs for OOM or disk errors",
          "code": "# Check kubelet logs\njournalctl -xe --unit kubelet --no-pager\nsystemctl status kubelet\n\n# Check container runtime\nsystemctl status containerd\ndocker info\n\n# Check cgroup driver mismatch\ncat /var/lib/kubelet/config.yaml | grep cgroupDriver\n\n# Check swap\nswapoff -a\nfree -h\n\n# Check disk space\ndf -h\n\n# Check memory\nfree -h\n# Check CPU\nnproc\n\n# Fix cgroup mismatch\necho 'apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ncgroupDriver: systemd' > /var/lib/kubelet/config.yaml\n\n# Restart services\nsystemctl daemon-reload\nsystemctl restart containerd\nsystemctl restart kubelet"
        },
        {
          "name": "QoS Classes & Eviction",
          "text": "# Critical pod got evicted - QoS classes and eviction\n\n**Question:** Critical pod got evicted due to node pressure. Explain QoS classes and eviction policies.\n\n## **Answer:**\n\nKubernetes uses QoS classes to determine eviction order:\n\n### **Three QoS Classes:**\n\n**1. Guaranteed (Highest Priority)**\n- All containers have CPU and memory limits AND requests\n- Limits equal requests for both CPU and memory\n- **Last to be evicted** (almost never evicted unless system daemons are threatened)\n\n**2. Burstable (Medium Priority)**\n- At least one container has CPU or memory request/limit\n- Does not meet Guaranteed criteria\n- Evicted after BestEffort but before Guaranteed\n- Within Burstable, pods exceeding requests are evicted first\n\n**3. BestEffort (Lowest Priority)**\n- No CPU or memory requests or limits set\n- **First to be evicted** under node pressure\n- Cannot trigger Cluster Autoscaler to add nodes\n\n### **Eviction Order:**\n1. BestEffort pods\n2. Burstable pods exceeding their requests\n3. Burstable pods below requests\n4. Guaranteed pods (only in extreme cases)\n\n### **Within each QoS tier**, eviction considers:\n- Pod Priority (PriorityClass)\n- Resource usage relative to requests\n\n### **Node-Pressure Eviction Thresholds:**\n- Memory pressure: `memory.available < threshold`\n- Disk pressure: `nodefs.available` or `imagefs.available < threshold`\n- PID pressure: `pid.available < threshold`\n\n### **Best Practices:**\n- Always set requests and limits for production workloads\n- Use Guaranteed QoS for critical services\n- Set appropriate PriorityClasses for mission-critical pods\n- Monitor node resource utilization to prevent evictions",
          "code": "# Example: Guaranteed QoS (highest priority)\nresources:\n  limits:\n    cpu: \"2\"\n    memory: 4Gi\n  requests:\n    cpu: \"2\"\n    memory: 4Gi\n\n# Example: Burstable QoS\nresources:\n  limits:\n    cpu: \"4\"\n    memory: 8Gi\n  requests:\n    cpu: \"2\"\n    memory: 4Gi\n\n# Example: BestEffort QoS (lowest priority)\n# No resources specified\n\n# Check pod QoS class\nkubectl get pod <pod-name> -o jsonpath='{.status.qosClass}'\n\n# Check evicted pods\nkubectl get pods --all-namespaces --field-selector=status.phase=Failed\n\n# Check node pressure\nkubectl top nodes\nkubectl describe node <node-name>\n\n# Example: PriorityClass for critical pods\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: critical\nvalue: 1000\nglobalDefault: false\ndescription: \"Critical priority pods\"\n\n# Use PriorityClass in deployment\nspec:\n  priorityClassName: critical"
        },
        {
          "name": "Rolling Update Downtime",
          "text": "# Rolling update caused downtime - probe configuration\n\n**Question:** A rolling update caused downtime. What went wrong in your readiness/startup probe or deployment config?\n\n## **Answer:**\n\nRolling updates require careful probe configuration to prevent downtime.\n\n### **Common Readiness Probe Failures:**\n\n**1. Probe Too Aggressive**\n- `initialDelaySeconds` too short—pod marked ready before application starts\n- `periodSeconds` too short—probe doesn't allow time for request processing\n- `failureThreshold` too low—temporary slowness causes pod removal from service\n\n**2. Startup Probe Missing**\nFor applications with long initialization, use startup probe to prevent premature readiness/liveness checks. Without startup probe, slow-starting apps may be killed before they're ready.\n\n### **Deployment Configuration Issues:**\n\n**Incorrect Rolling Update Strategy:**\n```yaml\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxUnavailable: 1  # Never allow all pods down\n    maxSurge: 1        # Create new before terminating old\n```\n\n**Missing `minReadySeconds`:** Prevents pods from being marked available too quickly.\n\n### **Root Causes of Downtime:**\n1. New pods marked ready before truly operational\n2. Old pods terminated before new pods ready\n3. Load balancer not updated quickly enough\n4. Application doesn't handle graceful shutdown (no preStop hook)\n5. Connection draining not configured\n\n### **Prevention:**\n- Configure readiness probe to accurately reflect application health\n- Use startup probe for slow-starting applications\n- Set appropriate `maxUnavailable: 0` for zero-downtime\n- Implement graceful shutdown with preStop hooks\n- Test rolling updates in staging environment first",
          "code": "# Example: Proper probe configuration\nreadinessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30  # Wait for app initialization\n  periodSeconds: 10         # Check every 10s\n  failureThreshold: 3       # Allow 3 failures before marking unready\n  successThreshold: 1\n\ntartupProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  failureThreshold: 30  # 30 * 10s = 5 minutes max startup time\n  periodSeconds: 10\n\n# Example: Rolling update with zero downtime\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxUnavailable: 0  # Never allow all pods down\n    maxSurge: 2        # Create new before terminating old\n\n# Example: minReadySeconds\nminReadySeconds: 10\n\n# Example: Graceful shutdown\nlifecycle:\n  preStop:\n    exec:\n      command:\n      - /bin/sh\n      - -c\n      - sleep 15  # Allow time for connection draining"
        },
        {
          "name": "Ingress Controller Scaling",
          "text": "# Ingress Controller fails under load\n\n**Question:** Ingress Controller fails under load. How do you debug and scale routing efficiently?\n\n## **Answer:**\n\n### **Debugging Under Load:**\n\n**1. Check Controller Pod Resources**\n```bash\nkubectl top pods -n ingress-nginx\nkubectl describe pod -n ingress-nginx <controller-pod>\n```\nLook for CPU/memory throttling or OOM kills. Ingress controllers can become CPU-bound under high request rates.\n\n**2. Review Ingress Controller Logs**\n```bash\nkubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n```\nLook for connection timeouts, upstream errors, or SSL/TLS issues.\n\n**3. Check Metrics**\n- Monitor request latency, error rates, and connection counts\n- Use Prometheus metrics exposed by NGINX ingress controller\n- Identify bottlenecks in request processing\n\n### **Scaling Strategies:**\n\n**1. Horizontal Scaling with HPA**\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ingress-nginx\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ingress-nginx-controller\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n**2. Vertical Scaling**\n- Increase CPU/memory requests and limits\n- Ensure sufficient resources for connection processing\n\n**3. Performance Tuning**\n- Configure worker processes and connections in NGINX\n- Implement caching for static content\n- Use rate limiting to protect backend services\n- Configure connection pooling and keepalive",
          "code": "# Check ingress controller resources\nkubectl top pods -n ingress-nginx\nkubectl describe pod -n ingress-nginx <controller-pod>\n\n# Check ingress logs\nkubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx\n\n# Scale ingress controller\nkubectl scale deployment ingress-nginx-controller --replicas=5 -n ingress-nginx\n\n# Example: HPA for ingress\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ingress-nginx\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ingress-nginx-controller\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n\n# Example: NGINX configuration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\ndata:\n  worker-processes: \"4\"\n  max-worker-connections: \"65536\"\n  keep-alive-requests: \"10000\""
        },
        {
          "name": "Istio Sidecar Performance",
          "text": "# Istio sidecar consumes more CPU than app\n\n**Question:** Istio sidecar consumes more CPU than your app. How do you profile and optimise mesh performance?\n\n## **Answer:**\n\nIstio proxies can consume significant resources; optimization is crucial.\n\n### **Profile Sidecar CPU Usage:**\n\n**Use pprof for CPU Profiling:**\n```bash\nkubectl port-forward -n <namespace> <pod-name> 15000:15000\ngo tool pprof -http=:8888 http://localhost:15000/debug/pprof/profile\n```\nThis captures 30 seconds of CPU usage and displays flame graphs showing where proxy spends CPU time.\n\n### **Optimization Techniques:**\n\n**1. Restrict Outbound Traffic with Sidecar Resources**\nThis is the **most effective optimization**—can reduce memory by 40-50% and improve CPU efficiency.\n\nBy default, Envoy receives configuration for ALL services in the mesh. Restricting outbound traffic reduces config size and processing overhead.\n\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: default\n  namespace: my-namespace\nspec:\n  egress:\n  - hosts:\n    - \"./*\"  # Only allow same namespace\n    - \"istio-system/*\"  # And Istio control plane\n```\n\n**2. Increase Worker Threads**\n```yaml\nmetadata:\n  annotations:\n    proxy.istio.io/config: |\n      concurrency: 4  # Match number of CPU cores\n```\n\n**3. Disable Unused Features**\n- Disable Mixer telemetry (migrate to Envoy native stats)\n- Reduce access logging verbosity\n- Disable protocol auto-detection if not needed\n\n**4. Switch to Native Envoy Filters**\n- Avoid WASM filters when possible (WASM has overhead)\n- Use native Envoy filters for better performance\n\n**5. Resource Allocation**\n- Set appropriate CPU/memory requests and limits for sidecars\n- Monitor actual usage and adjust accordingly\n\n### **Architecture Decisions:**\n- Evaluate if ambient mesh reduces overhead for specific workloads\n- Use headless services where sidecar isn't needed\n- Consider sidecar-free communication for high-throughput internal services",
          "code": "# Profile sidecar CPU usage\nkubectl port-forward -n <namespace> <pod-name> 15000:15000\ngo tool pprof -http=:8888 http://localhost:15000/debug/pprof/profile\n\n# Check Envoy stats\nkubectl exec -it <pod-name> -c istio-proxy -- curl localhost:15000/stats/prometheus\n\n# Example: Restrict sidecar outbound traffic\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: default\n  namespace: my-namespace\nspec:\n  egress:\n  - hosts:\n    - \"./*\"  # Only allow same namespace\n    - \"istio-system/*\"  # And Istio control plane\n\n# Example: Increase worker threads\nmetadata:\n  annotations:\n    proxy.istio.io/config: |\n      concurrency: 4  # Match number of CPU cores\n\n# Monitor Istio proxy metrics\nkubectl exec -it <pod-name> -c istio-proxy -- curl localhost:15000/stats"
        },
        {
          "name": "etcd Performance Tuning",
          "text": "# etcd is slowing down control plane ops\n\n**Question:** etcd is slowing down control plane ops. Root causes + how do you tune it safely?\n\n## **Answer:**\n\netcd performance directly impacts cluster responsiveness.\n\n### **Root Causes of Slow etcd:**\n\n**1. Slow Disk I/O**\n- etcd writes to disk for every operation\n- Non-SSD or slow SSD causes high fsync latencies\n- Shared disk with other workloads causes contention\n\n**2. Network Latency Between Members**\n- etcd is highly sensitive to network latency\n- Members in different availability zones increase commit times\n- High RTT affects leader election and consensus\n\n**3. Large Database Size**\n- Large databases slow down operations\n- Defragmentation takes longer on large databases\n- More objects mean more data to sync\n\n**4. Insufficient Resources**\n- CPU/memory limits too low for workload\n- Resource contention on etcd hosts\n\n**5. High Write Rate**\n- Too many watch streams\n- Excessive API operations (too many objects, high churn rate)\n\n### **Safe Tuning Approaches:**\n\n**1. Optimize Disk Performance**\n- **Use fast NVMe SSDs** (most critical)\n- Dedicate disk exclusively to etcd\n- Use separate disk for WAL and data\n\n**2. Reduce Network Latency**\n- Place etcd members in the same availability zone\n- Use low-latency network connections\n- Monitor RTT between members\n\n**3. Database Maintenance**\n- **Regular compaction**: Remove old revisions\n- **Defragmentation**: Reclaim space after compaction\n- **Monitor database size** and set alarms\n\n```bash\n# Automatic compaction every hour for revisions older than 5 minutes\n--auto-compaction-mode=periodic\n--auto-compaction-retention=5m\n\n# Defragmentation\netcdctl defrag\n```\n\n**4. Tune Heartbeat and Election Timeouts**\n- Adjust based on hardware speed\n- Slower hardware needs longer timeouts\n- Default heartbeat: 100ms, election timeout: 1000ms\n\n**5. Configure Backend Quota**\n```bash\n--quota-backend-bytes=8589934592  # 8GB limit\n```\nPrevents database from growing unbounded.\n\n### **Monitoring:**\nKey metrics to watch:\n- WAL fsync duration (should be < 10ms)\n- Backend commit duration\n- Leader changes (should be rare)\n- Database size\n- Network RTT between members",
          "code": "# Check etcd database size\netcdctl endpoint status\n\n# Defragment etcd\netcdctl defrag\n\n# Check etcd health\netcdctl endpoint health\n\n# Example: Compaction configuration\n--auto-compaction-mode=periodic\n--auto-compaction-retention=5m\n\n# Example: Backend quota\n--quota-backend-bytes=8589934592  # 8GB limit\n\n# Monitor etcd performance\n# WAL fsync duration should be < 10ms\n# Backend commit duration should be < 1ms\n# Leader changes should be rare\n\n# Check etcd metrics in Prometheus\n# wal_fsync_duration_seconds\n# backend_commit_duration_seconds\n# leader_changes"
        },
        {
          "name": "Image Policy Enforcement",
          "text": "# Enforce images from trusted registry only\n\n**Question:** You must enforce images from a trusted internal registry only. Gatekeeper, Kyverno, or custom Admission Webhook – what's your move?\n\n## **Answer:**\n\nModern policy enforcement should use declarative tools.\n\n### **Recommendation: Kyverno**\n\n**Why Kyverno:**\n- **Native Kubernetes YAML syntax**—no need to learn Rego\n- Simpler to write and maintain policies\n- Good CLI for testing policies before deployment\n- Excellent for image validation use cases\n- Built-in mutation capabilities\n\n### **Example Kyverno Policy:**\n```yaml\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-image-registries\nspec:\n  validationFailureAction: Enforce\n  background: true\n  rules:\n  - name: validate-registries\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"Images must come from registry.company.com\"\n      pattern:\n        spec:\n          containers:\n          - image: \"registry.company.com/*\"\n          initContainers:\n          - image: \"registry.company.com/*\"\n```\n\n### **Alternative: OPA Gatekeeper**\n\n**When to Choose Gatekeeper:**\n- Already using OPA elsewhere in infrastructure\n- Need complex logic and constraints\n- Want standardized policy language (Rego) across tools\n- Require audit mode and policy reports\n\n### **Custom Admission Webhook**\n**Only use when:**\n- Extremely complex validation logic not supported by Kyverno/Gatekeeper\n- Need to call external systems for validation\n- Require custom integration with internal tools\n\n**Downsides**: More operational overhead, requires maintaining custom code, harder to audit.\n\n### **Best Practices:**\n- Test policies in audit/dry-run mode first\n- Use policy CLI tools for validation before applying\n- Monitor policy violations and denials\n- Document policy intent and exceptions\n- Version control all policies",
          "code": "# Example: Kyverno policy to restrict image registries\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-image-registries\nspec:\n  validationFailureAction: Enforce\n  background: true\n  rules:\n  - name: validate-registries\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"Images must come from registry.company.com\"\n      pattern:\n        spec:\n          containers:\n          - image: \"registry.company.com/*\"\n\n# Test policy with Kyverno CLI\nkyverno apply policy.yaml --resource test-deployment.yaml\n\n# Verify policy is valid\nkyverno validate policy.yaml\n\n# Example: OPA Gatekeeper policy\napiVersion: templates.gatekeeper.sh/v1beta1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sallowedrepos\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sAllowedRepos\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8sallowedrepos\n        violation[{\"msg\": msg}] {\n          container := input.review.object.spec.containers[_]\n          not strings.any_prefix_match(container.image, input.parameters.repos)\n          msg := sprintf(\"Image '%v' not from trusted registry\", [container.image])\n        }"
        },
        {
          "name": "ContainerCreating Debugging",
          "text": "# Pods stuck in ContainerCreating forever\n\n**Question:** Pods stuck in ContainerCreating forever. CNI attach delay? OverlayFS corruption? Walk me through your root-cause process.\n\n## **Answer:**\n\nSystematic approach to diagnose ContainerCreating issues.\n\n### **Step 1: Describe Pod for Events**\n```bash\nkubectl describe pod <pod-name>\n```\nLook for specific error messages:\n- `FailedCreatePodSandBox`: Network or container runtime issue\n- `Failed to create pod sandbox`: CNI failure\n- `ImagePullBackOff`: Image retrieval issue\n- `Volume mount errors`: Storage issues\n\n### **Step 2: Identify Error Category**\n\n**CNI/Network Issues:**\n```bash\n# Check CNI pod status\nkubectl get pods -n kube-system -l k8s-app=<cni-name>\n\n# Check CNI logs\nkubectl logs -n kube-system -l k8s-app=calico-node\n\n# On the node, check CNI binary and config\nls -la /opt/cni/bin/\ncat /etc/cni/net.d/*\n```\nCommon CNI Issues:\n- CNI pods not running\n- IP address exhaustion (no IPs left in CIDR range)\n- Network plugin misconfiguration\n- CNI binary missing or corrupted\n\n**Container Runtime Issues:**\n```bash\nsystemctl status containerd\njournalctl -u containerd -n 50\n```\nContainer runtime (containerd/Docker) not responding or overwhelmed.\n\n**Storage/Volume Mount Failures:**\n```bash\nkubectl get pvc\nkubectl describe pvc <pvc-name>\n```\nPVC not bound, StorageClass misconfigured, or volume provisioner failing.\n\n**Node Resource Exhaustion:**\n```bash\nkubectl describe node <node-name>\ndf -h  # on node\n```\nDisk space full, too many pods on node (IP exhaustion), or memory/CPU limits reached.\n\n### **Step 3: Advanced Debugging**\n\n**Check kubelet logs on the node:**\n```bash\njournalctl -u kubelet -n 100 --no-pager\n```\n\n**OverlayFS Corruption Detection:**\n- Check node kernel logs for OverlayFS errors\n- Look for I/O errors in dmesg\n- Run filesystem checks on node\n\n```bash\ndmesg | grep -i overlay\ndmesg | grep -i error\n```\n\n### **Step 4: Resolution Paths:**\n- **CNI issue**: Restart CNI DaemonSet or redeploy CNI\n- **Image issue**: Fix registry connectivity, increase timeout, use image cache\n- **Runtime issue**: Restart container runtime service\n- **Storage issue**: Fix PVC/PV binding, check provisioner\n- **Resource issue**: Add nodes, scale down workloads, clean up resources\n- **OverlayFS corruption**: Reboot node, repair filesystem, replace node",
          "code": "# Debug ContainerCreating issues\nkubectl describe pod <pod-name>\n\n# Check CNI status\nkubectl get pods -n kube-system -l k8s-app=calico-node\nkubectl logs -n kube-system -l k8s-app=calico-node\n\n# Check on node\nls -la /opt/cni/bin/\ncat /etc/cni/net.d/*\n\n# Check container runtime\nsystemctl status containerd\njournalctl -u containerd -n 50\n\n# Check kubelet logs on node\njournalctl -u kubelet -n 100 --no-pager\n\n# Check storage\nkubectl get pvc\nkubectl describe pvc <pvc-name>\n\n# Check node resources\nkubectl describe node <node-name>\ndf -h\nfree -h\n\n# Check for OverlayFS corruption\ndmesg | grep -i overlay\ndmesg | grep -i error\n\n# Restart CNI if needed\nkubectl rollout restart daemonset <cni-daemonset> -n kube-system\n\n# Check for IP exhaustion\nkubectl get pods --all-namespaces -o wide | wc -l\n\n# Check container runtime on node\ncrictl ps -a\ncrictl images"
        },
        {
          "name": "DNS Failures Debugging",
          "text": "# Random DNS failures in Pods\n\n**Question:** Random DNS failures in Pods. How do you debug CoreDNS, kube-proxy, and conntrack interactions?\n\n## **Answer:**\n\nDNS issues in Kubernetes are complex due to multiple layers.\n\n### **Step 1: Verify CoreDNS is Running**\n```bash\nkubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=kube-dns\n```\nCheck for:\n- CoreDNS pods running and ready\n- No errors in logs (I/O timeouts, connection refused)\n- Sufficient replicas to handle load\n\n### **Step 2: Test DNS Resolution from Pod**\n```bash\nkubectl run -it --rm debug --image=busybox --restart=Never -- nslookup kubernetes.default\n```\nIf this fails, DNS service is broken. If it succeeds, issue is application-specific.\n\n### **Step 3: Check kube-dns Service**\n```bash\nkubectl get svc -n kube-system kube-dns\nkubectl get endpoints -n kube-system kube-dns\n```\nVerify ClusterIP is assigned and endpoints are present.\n\n### **Step 4: Debug Common Issues**\n\n**Conntrack Table Exhaustion (Very Common):**\nUDP connections create conntrack entries. High QPS can fill conntrack table, causing random DNS timeouts and failures.\n\n```bash\n# On node\nsysctl net.netfilter.nf_conntrack_count\nsysctl net.netfilter.nf_conntrack_max\n\n# Check for \"table full\" errors\ndmesg | grep conntrack\njournalctl -k | grep conntrack\n```\n\n**Fix:**\n```bash\n# Increase conntrack table size\nsysctl -w net.netfilter.nf_conntrack_max=1000000\nsysctl -w net.netfilter.nf_conntrack_buckets=250000\n\n# Make permanent in /etc/sysctl.conf\n```\n\n**Stale Conntrack Entries:**\nAfter CoreDNS pod restart, old connections may remain in conntrack, causing pods to connect to terminated CoreDNS IPs.\n\n**Fix:**\n```bash\n# Flush conntrack table (disruptive—do carefully)\nconntrack -F\n```\n\n**kube-proxy Issues:**\nkube-proxy not running or misconfigured, IPVS mode issues with Service updates.\n\n**CoreDNS Configuration Issues:**\nCheck CoreDNS ConfigMap for misconfigurations, verify upstream DNS servers are reachable, check for loops in configuration.\n\n**ndots Configuration Problem:**\nDefault ndots=5 causes multiple DNS queries per request, can overload CoreDNS or cause delays.\n\n### **Step 5: Enable CoreDNS Logging**\nAdd `log` plugin to Corefile:\n```\n.:53 {\n    log\n    errors\n    health\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\n        pods insecure\n        fallthrough in-addr.arpa ip6.arpa\n    }\n}\n```\n\n### **Step 6: Monitor and Tune**\n- Scale CoreDNS replicas based on QPS\n- Use NodeLocal DNSCache to reduce CoreDNS load\n- Monitor DNS query latency and error rates\n- Consider DNS caching at application level",
          "code": "# Verify CoreDNS is running\nkubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl logs -n kube-system -l k8s-app=kube-dns\n\n# Test DNS resolution\nkubectl run -it --rm debug --image=busybox --restart=Never -- nslookup kubernetes.default\n\n# Check DNS service\nkubectl get svc -n kube-system kube-dns\nkubectl get endpoints -n kube-system kube-dns\n\n# Check conntrack table\nsysctl net.netfilter.nf_conntrack_count\nsysctl net.netfilter.nf_conntrack_max\n\n# Check for conntrack errors\ndmesg | grep conntrack\njournalctl -k | grep conntrack\n\n# Fix conntrack table exhaustion\nsysctl -w net.netfilter.nf_conntrack_max=1000000\nsysctl -w net.netfilter.nf_conntrack_buckets=250000\n\n# Flush stale conntrack entries\nconntrack -F\n\n# Check kube-proxy\nkubectl get pods -n kube-system -l k8s-app=kube-proxy\nkubectl logs -n kube-system -l k8s-app=kube-proxy\n\n# Check CoreDNS configuration\nkubectl get configmap coredns -n kube-system -o yaml\n\n# Enable CoreDNS logging\nkubectl edit configmap coredns -n kube-system\n# Add 'log' plugin to Corefile\n\n# Scale CoreDNS if needed\nkubectl scale deployment coredns --replicas=3 -n kube-system"
        }
      ]
    },
    {
      "id": "security-qa",
      "title": "🔒 Security Q&A",
      "sections": [
        {
          "name": "Authentication & Authorization",
          "text": "# Authentication & Authorization\n\n**Question:** How would you implement secure authentication and authorization in a microservices architecture?\n\n## **Answer:**\n\n### **Authentication Strategies**\n\n#### **1. JWT (JSON Web Tokens)**\n- **Stateless**: No server-side session storage\n- **Self-Contained**: Contains user information and permissions\n- **Scalable**: Easy to scale across multiple services\n- **Security**: Signed and optionally encrypted\n\n#### **2. OAuth 2.0**\n- **Authorization Framework**: Industry standard for authorization\n- **Multiple Flows**: Authorization Code, Client Credentials, etc.\n- **Third-Party Integration**: Easy integration with external providers\n- **Granular Permissions**: Fine-grained access control\n\n#### **3. OIDC (OpenID Connect)**\n- **Identity Layer**: Built on top of OAuth 2.0\n- **Standardized**: Industry standard for authentication\n- **User Info**: Provides user information\n- **Single Sign-On**: Enables SSO across applications\n\n### **Authorization Patterns**\n\n#### **1. RBAC (Role-Based Access Control)**\n- **Roles**: Users are assigned roles\n- **Permissions**: Roles have specific permissions\n- **Hierarchical**: Roles can inherit from other roles\n- **Simple**: Easy to understand and implement\n\n#### **2. ABAC (Attribute-Based Access Control)**\n- **Attributes**: Decisions based on user, resource, and environment attributes\n- **Flexible**: More flexible than RBAC\n- **Complex**: More complex to implement\n- **Context-Aware**: Can consider context in decisions\n\n#### **3. Policy-Based Access Control**\n- **Policies**: Centralized policy definitions\n- **Externalized**: Policies defined outside application code\n- **Dynamic**: Policies can be changed without code changes\n- **Audit**: Easy to audit access decisions\n\n### **Security Best Practices**\n- **HTTPS**: Always use HTTPS in production\n- **Token Expiration**: Set appropriate token expiration times\n- **Refresh Tokens**: Use refresh tokens for long-lived sessions\n- **Rate Limiting**: Implement rate limiting to prevent abuse\n- **Input Validation**: Validate all inputs\n- **SQL Injection Prevention**: Use parameterized queries\n- **XSS Prevention**: Sanitize user inputs\n- **CSRF Protection**: Implement CSRF protection\n\n### **Microservices Security**\n- **Service-to-Service**: Secure communication between services\n- **mTLS**: Mutual TLS for service authentication\n- **API Gateway**: Centralized security enforcement\n- **Service Mesh**: Security policies in service mesh\n- **Secrets Management**: Secure storage of secrets\n- **Network Policies**: Network-level security policies",
          "code": "// Example: JWT Authentication (Spring Boot)\n@RestController\npublic class AuthController {\n    \n    @Autowired\n    private JwtTokenProvider jwtTokenProvider;\n    \n    @PostMapping(\"/login\")\n    public ResponseEntity<AuthResponse> login(@RequestBody LoginRequest request) {\n        // Authenticate user\n        UserDetails userDetails = userService.loadUserByUsername(request.getUsername());\n        \n        if (!passwordEncoder.matches(request.getPassword(), userDetails.getPassword())) {\n            throw new BadCredentialsException(\"Invalid credentials\");\n        }\n        \n        // Generate JWT token\n        String token = jwtTokenProvider.generateToken(userDetails);\n        String refreshToken = jwtTokenProvider.generateRefreshToken(userDetails);\n        \n        return ResponseEntity.ok(new AuthResponse(token, refreshToken));\n    }\n    \n    @PostMapping(\"/refresh\")\n    public ResponseEntity<AuthResponse> refresh(@RequestBody RefreshTokenRequest request) {\n        String refreshToken = request.getRefreshToken();\n        \n        if (jwtTokenProvider.validateToken(refreshToken)) {\n            String username = jwtTokenProvider.getUsernameFromToken(refreshToken);\n            UserDetails userDetails = userService.loadUserByUsername(username);\n            \n            String newToken = jwtTokenProvider.generateToken(userDetails);\n            String newRefreshToken = jwtTokenProvider.generateRefreshToken(userDetails);\n            \n            return ResponseEntity.ok(new AuthResponse(newToken, newRefreshToken));\n        }\n        \n        throw new BadCredentialsException(\"Invalid refresh token\");\n    }\n}\n\n// Example: JWT Token Provider\n@Component\npublic class JwtTokenProvider {\n    \n    private final String secretKey = \"mySecretKey\";\n    private final long validityInMilliseconds = 3600000; // 1 hour\n    private final long refreshValidityInMilliseconds = 86400000; // 24 hours\n    \n    public String generateToken(UserDetails userDetails) {\n        Date now = new Date();\n        Date validity = new Date(now.getTime() + validityInMilliseconds);\n        \n        return Jwts.builder()\n            .setSubject(userDetails.getUsername())\n            .setIssuedAt(now)\n            .setExpiration(validity)\n            .claim(\"roles\", userDetails.getAuthorities())\n            .signWith(SignatureAlgorithm.HS256, secretKey)\n            .compact();\n    }\n    \n    public boolean validateToken(String token) {\n        try {\n            Jwts.parser().setSigningKey(secretKey).parseClaimsJws(token);\n            return true;\n        } catch (JwtException | IllegalArgumentException e) {\n            return false;\n        }\n    }\n    \n    public String getUsernameFromToken(String token) {\n        return Jwts.parser()\n            .setSigningKey(secretKey)\n            .parseClaimsJws(token)\n            .getBody()\n            .getSubject();\n    }\n}\n\n// Example: OAuth 2.0 Resource Server\n@Configuration\n@EnableResourceServer\npublic class ResourceServerConfig extends ResourceServerConfigurerAdapter {\n    \n    @Override\n    public void configure(HttpSecurity http) throws Exception {\n        http\n            .authorizeRequests()\n            .antMatchers(\"/api/public/**\").permitAll()\n            .antMatchers(\"/api/admin/**\").hasRole(\"ADMIN\")\n            .antMatchers(\"/api/user/**\").hasRole(\"USER\")\n            .anyRequest().authenticated()\n            .and()\n            .oauth2ResourceServer()\n            .jwt();\n    }\n}\n\n// Example: RBAC Implementation\n@Entity\npublic class User {\n    @Id\n    private String id;\n    private String username;\n    private String email;\n    \n    @ManyToMany(fetch = FetchType.EAGER)\n    private Set<Role> roles = new HashSet<>();\n}\n\n@Entity\npublic class Role {\n    @Id\n    private String id;\n    private String name;\n    \n    @ManyToMany(fetch = FetchType.EAGER)\n    private Set<Permission> permissions = new HashSet<>();\n}\n\n@Entity\npublic class Permission {\n    @Id\n    private String id;\n    private String name;\n    private String resource;\n    private String action;\n}\n\n// Example: Service-to-Service Authentication (mTLS)\n@Configuration\npublic class SecurityConfig {\n    \n    @Bean\n    public RestTemplate restTemplate() {\n        try {\n            // Load client certificate and key\n            KeyStore keyStore = KeyStore.getInstance(\"PKCS12\");\n            keyStore.load(new FileInputStream(\"client.p12\"), \"password\".toCharArray());\n            \n            // Load trust store\n            KeyStore trustStore = KeyStore.getInstance(\"PKCS12\");\n            trustStore.load(new FileInputStream(\"truststore.p12\"), \"password\".toCharArray());\n            \n            // Create SSL context\n            SSLContext sslContext = SSLContextBuilder.create()\n                .loadKeyMaterial(keyStore, \"password\".toCharArray())\n                .loadTrustMaterial(trustStore, null)\n                .build();\n            \n            // Create HTTP client with SSL context\n            CloseableHttpClient httpClient = HttpClients.custom()\n                .setSSLContext(sslContext)\n                .build();\n            \n            HttpComponentsClientHttpRequestFactory factory = \n                new HttpComponentsClientHttpRequestFactory(httpClient);\n            \n            return new RestTemplate(factory);\n            \n        } catch (Exception e) {\n            throw new RuntimeException(\"Failed to create RestTemplate\", e);\n        }\n    }\n}"
        },
        {
          "name": "Data Security",
          "text": "# Data Security & Privacy\n\n**Question:** How do you ensure data security and privacy in a distributed system?\n\n## **Answer:**\n\n### **Data Encryption**\n\n#### **1. Encryption at Rest**\n- **Database Encryption**: Encrypt sensitive data in databases\n- **File System Encryption**: Encrypt files on disk\n- **Key Management**: Secure key storage and rotation\n- **Transparent Encryption**: Application-transparent encryption\n\n#### **2. Encryption in Transit**\n- **HTTPS/TLS**: Encrypt all network communication\n- **mTLS**: Mutual TLS for service-to-service communication\n- **VPN**: Virtual private networks for secure connections\n- **Certificate Management**: Proper certificate lifecycle management\n\n#### **3. Application-Level Encryption**\n- **Field-Level Encryption**: Encrypt specific fields\n- **Symmetric Encryption**: Use AES for bulk data encryption\n- **Asymmetric Encryption**: Use RSA for key exchange\n- **Hybrid Encryption**: Combine symmetric and asymmetric encryption\n\n### **Data Privacy**\n\n#### **1. Data Classification**\n- **Public Data**: No restrictions\n- **Internal Data**: Company-internal use only\n- **Confidential Data**: Restricted access\n- **Personal Data**: Subject to privacy regulations\n\n#### **2. Data Minimization**\n- **Collect Only What's Needed**: Minimize data collection\n- **Retention Policies**: Define data retention periods\n- **Data Deletion**: Secure data deletion\n- **Anonymization**: Remove personally identifiable information\n\n#### **3. Privacy Regulations**\n- **GDPR**: General Data Protection Regulation\n- **CCPA**: California Consumer Privacy Act\n- **HIPAA**: Health Insurance Portability and Accountability Act\n- **SOX**: Sarbanes-Oxley Act\n\n### **Access Control**\n- **Principle of Least Privilege**: Grant minimum necessary access\n- **Role-Based Access**: Assign access based on roles\n- **Attribute-Based Access**: Use attributes for access decisions\n- **Dynamic Access Control**: Context-aware access control\n\n### **Audit and Monitoring**\n- **Access Logging**: Log all data access\n- **Data Lineage**: Track data flow and transformations\n- **Compliance Monitoring**: Monitor compliance with regulations\n- **Incident Response**: Respond to security incidents\n\n### **Data Loss Prevention**\n- **DLP Solutions**: Implement data loss prevention tools\n- **Content Inspection**: Inspect data in transit and at rest\n- **Policy Enforcement**: Enforce data handling policies\n- **User Training**: Train users on data security\n\n### **Secure Development**\n- **Secure Coding**: Follow secure coding practices\n- **Code Review**: Review code for security issues\n- **Static Analysis**: Use static analysis tools\n- **Penetration Testing**: Regular security testing",
          "code": "// Example: Field-Level Encryption\n@Service\npublic class UserService {\n    \n    @Autowired\n    private AESUtil aesUtil;\n    \n    public User createUser(UserRequest request) {\n        User user = new User();\n        user.setUsername(request.getUsername());\n        \n        // Encrypt sensitive fields\n        user.setEmail(aesUtil.encrypt(request.getEmail()));\n        user.setPhoneNumber(aesUtil.encrypt(request.getPhoneNumber()));\n        user.setSsn(aesUtil.encrypt(request.getSsn()));\n        \n        return userRepository.save(user);\n    }\n    \n    public User getUser(String userId) {\n        User user = userRepository.findById(userId);\n        \n        // Decrypt sensitive fields\n        user.setEmail(aesUtil.decrypt(user.getEmail()));\n        user.setPhoneNumber(aesUtil.decrypt(user.getPhoneNumber()));\n        user.setSsn(aesUtil.decrypt(user.getSsn()));\n        \n        return user;\n    }\n}\n\n// Example: AES Encryption Utility\n@Component\npublic class AESUtil {\n    \n    private final String secretKey = \"MySecretKey12345\";\n    private final String salt = \"MySalt12345\";\n    \n    public String encrypt(String plainText) {\n        try {\n            SecretKeySpec keySpec = new SecretKeySpec(secretKey.getBytes(), \"AES\");\n            Cipher cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n            cipher.init(Cipher.ENCRYPT_MODE, keySpec);\n            \n            byte[] encrypted = cipher.doFinal(plainText.getBytes());\n            return Base64.getEncoder().encodeToString(encrypted);\n            \n        } catch (Exception e) {\n            throw new RuntimeException(\"Encryption failed\", e);\n        }\n    }\n    \n    public String decrypt(String encryptedText) {\n        try {\n            SecretKeySpec keySpec = new SecretKeySpec(secretKey.getBytes(), \"AES\");\n            Cipher cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n            cipher.init(Cipher.DECRYPT_MODE, keySpec);\n            \n            byte[] decrypted = cipher.doFinal(Base64.getDecoder().decode(encryptedText));\n            return new String(decrypted);\n            \n        } catch (Exception e) {\n            throw new RuntimeException(\"Decryption failed\", e);\n        }\n    }\n}\n\n// Example: Data Anonymization\n@Service\npublic class DataAnonymizationService {\n    \n    public User anonymizeUser(User user) {\n        User anonymizedUser = new User();\n        anonymizedUser.setId(user.getId());\n        anonymizedUser.setUsername(user.getUsername());\n        \n        // Anonymize sensitive fields\n        anonymizedUser.setEmail(anonymizeEmail(user.getEmail()));\n        anonymizedUser.setPhoneNumber(anonymizePhone(user.getPhoneNumber()));\n        anonymizedUser.setSsn(anonymizeSsn(user.getSsn()));\n        \n        return anonymizedUser;\n    }\n    \n    private String anonymizeEmail(String email) {\n        String[] parts = email.split(\"@\");\n        String username = parts[0];\n        String domain = parts[1];\n        \n        return username.substring(0, 2) + \"***@\" + domain;\n    }\n    \n    private String anonymizePhone(String phone) {\n        return phone.substring(0, 3) + \"-***-\" + phone.substring(phone.length() - 4);\n    }\n    \n    private String anonymizeSsn(String ssn) {\n        return \"***-**-\" + ssn.substring(ssn.length() - 4);\n    }\n}\n\n// Example: Audit Logging\n@Component\npublic class AuditLogger {\n    \n    @Autowired\n    private AuditRepository auditRepository;\n    \n    public void logDataAccess(String userId, String resource, String action, String data) {\n        AuditLog auditLog = new AuditLog();\n        auditLog.setUserId(userId);\n        auditLog.setResource(resource);\n        auditLog.setAction(action);\n        auditLog.setData(data);\n        auditLog.setTimestamp(LocalDateTime.now());\n        auditLog.setIpAddress(getClientIpAddress());\n        \n        auditRepository.save(auditLog);\n    }\n    \n    @EventListener\n    public void handleDataAccess(DataAccessEvent event) {\n        logDataAccess(\n            event.getUserId(),\n            event.getResource(),\n            event.getAction(),\n            event.getData()\n        );\n    }\n}\n\n// Example: Data Retention Policy\n@Service\npublic class DataRetentionService {\n    \n    @Autowired\n    private UserRepository userRepository;\n    \n    @Scheduled(cron = \"0 0 2 * * ?\") // Run daily at 2 AM\n    public void cleanupExpiredData() {\n        LocalDateTime cutoffDate = LocalDateTime.now().minusYears(7);\n        \n        List<User> expiredUsers = userRepository.findByLastLoginBefore(cutoffDate);\n        \n        for (User user : expiredUsers) {\n            // Anonymize user data\n            user.setEmail(\"deleted@example.com\");\n            user.setPhoneNumber(\"000-000-0000\");\n            user.setSsn(\"000-00-0000\");\n            user.setActive(false);\n            \n            userRepository.save(user);\n        }\n    }\n}"
        }
      ]
    }
  ]
}
