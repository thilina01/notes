{
  "config": {
    "id": "postgresql-advanced",
    "name": "PostgreSQL Advanced",
    "description": "Advanced PostgreSQL concepts, optimization, and administration",
    "icon": "ðŸ˜",
    "enabled": true
  },
  "content": [
    {
      "id": "postgresql-indexing",
      "title": "ðŸ—„ï¸ PostgreSQL Indexing â€” Performance Optimization",
      "sections": [
        {
          "name": "Index Types and Strategies",
          "text": "PostgreSQL offers various index types optimized for different use cases. Understanding when to use each type is crucial for optimal performance.\n\n**B-Tree Index**: Default and most versatile. Use for equality, range queries, and sorting. Perfect for primary keys, foreign keys, and frequently queried columns.\n\n**Composite Index**: Multiple columns in one index. Use when queries filter by multiple columns. Order matters - put most selective columns first.\n\n**Partial Index**: Only indexes rows matching a condition. Saves space and improves performance when you only query a subset of data.\n\n**Covering Index**: Includes all columns needed for a query. Eliminates table lookups, dramatically improving performance for read-heavy workloads.\n\n**Expression Index**: Indexes computed values. Use when queries filter by calculated columns or functions.\n\n**Hash Index**: Only for equality comparisons. Faster than B-tree for exact matches but can't handle ranges or sorting.\n\n**GIN Index**: For complex data types (arrays, JSON, full-text). Excellent for searching within arrays or JSON documents.\n\n**GiST Index**: For geometric and custom data types. Supports various operators and is extensible.\n\n**BRIN Index**: For very large tables with natural ordering. Minimal storage overhead, good for time-series data.",
          "code": "-- B-Tree Index (Most Common) - Use for: equality, ranges, sorting\n-- Benefits: Versatile, supports all comparison operators\n-- When to use: Primary keys, foreign keys, frequently queried columns\nCREATE INDEX idx_customer_email ON customers(email);\nCREATE INDEX idx_orders_date ON orders(order_date);\n\n-- Composite Index - Use for: multi-column queries\n-- Benefits: Single index covers multiple columns, faster than multiple indexes\n-- When to use: Queries that filter by multiple columns together\n-- Order matters: Most selective column first\nCREATE INDEX idx_customer_name_email ON customers(last_name, first_name, email);\n\n-- Partial Index (PostgreSQL specific) - Use for: subset of data\n-- Benefits: Smaller index size, faster queries, less maintenance\n-- When to use: Queries only access a subset of rows (e.g., active users only)\nCREATE INDEX idx_active_customers ON customers(email) WHERE status = 'active';\n\n-- Covering Index - Use for: read-heavy workloads\n-- Benefits: Eliminates table lookups, much faster SELECT queries\n-- When to use: Queries that only need indexed columns\nCREATE INDEX idx_order_summary ON orders(customer_id, order_date, total_amount);\n\n-- Unique Index - Use for: data integrity\n-- Benefits: Prevents duplicates, automatically creates constraint\n-- When to use: Columns that must be unique (emails, usernames)\nCREATE UNIQUE INDEX idx_customer_email_unique ON customers(email);\n\n-- Index on Expression - Use for: computed columns\n-- Benefits: Fast queries on calculated values\n-- When to use: Queries filter by functions or expressions\nCREATE INDEX idx_customer_name_lower ON customers(LOWER(last_name));\n\n-- Hash Index - Use for: exact equality only\n-- Benefits: Faster than B-tree for exact matches\n-- When to use: Simple equality lookups, no ranges needed\nCREATE INDEX idx_customer_id_hash ON customers USING hash(id);\n\n-- GIN Index - Use for: complex data types\n-- Benefits: Fast searches in arrays, JSON, full-text\n-- When to use: Array operations, JSON queries, full-text search\nCREATE INDEX idx_customer_tags_gin ON customers USING gin(tags);\n\n-- GiST Index - Use for: geometric data\n-- Benefits: Supports various geometric operators\n-- When to use: Geographic data, custom data types\nCREATE INDEX idx_locations_gist ON locations USING gist(coordinates);\n\n-- BRIN Index - Use for: very large tables\n-- Benefits: Minimal storage, good for time-series\n-- When to use: Large tables with natural ordering (time-series data)\nCREATE INDEX idx_orders_date_brin ON orders USING brin(order_date);"
        },
        {
          "name": "Index Performance Analysis",
          "text": "Analyze index usage and performance to optimize PostgreSQL queries and identify missing or unused indexes.\n\n**Why Monitor Index Usage?**\n- **Identify Unused Indexes**: Remove indexes that consume space but aren't used\n- **Find Missing Indexes**: Discover queries that would benefit from new indexes\n- **Optimize Performance**: Understand which indexes are most valuable\n- **Cost-Benefit Analysis**: Balance query speed vs. storage and maintenance costs\n\n**Key Metrics to Monitor:**\n- **idx_scan**: Number of times index was used (higher = more valuable)\n- **idx_tup_read**: Rows read from index (indicates selectivity)\n- **idx_tup_fetch**: Rows fetched from table (indicates index efficiency)\n\n**Performance Analysis Benefits:**\n- **Storage Optimization**: Remove unused indexes to save space\n- **Query Optimization**: Add missing indexes for slow queries\n- **Maintenance Planning**: Focus on frequently used indexes\n- **Cost Reduction**: Eliminate unnecessary index maintenance overhead",
          "code": "-- Check index usage - Shows which indexes are actually being used\n-- High idx_scan = frequently used index (valuable)\n-- Low idx_scan = rarely used index (candidate for removal)\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n\n-- Find unused indexes - Candidates for removal\n-- Benefits: Free up storage space, reduce maintenance overhead\n-- When to remove: Indexes with 0 scans and no unique constraints\nSELECT \n    schemaname,\n    tablename,\n    indexname\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0;\n\n-- Analyze query execution plan - Shows how PostgreSQL uses indexes\n-- ANALYZE: Shows actual execution time and row counts\n-- BUFFERS: Shows disk I/O statistics\n-- Use this to verify index usage and identify bottlenecks\nEXPLAIN (ANALYZE, BUFFERS) \nSELECT * FROM customers \nWHERE email = 'john@example.com';\n\n-- Check index size - Monitor storage usage\n-- Large indexes consume more storage and take longer to maintain\n-- Consider partial indexes or covering indexes for large tables\nSELECT \n    indexname,\n    pg_size_pretty(pg_relation_size(indexname::regclass)) as size\nFROM pg_indexes \nWHERE tablename = 'customers';\n\n-- Detailed index statistics - Comprehensive analysis\n-- Combines usage stats with size information\n-- Helps prioritize index maintenance and optimization\nSELECT \n    i.indexname,\n    i.tablename,\n    s.idx_scan,\n    s.idx_tup_read,\n    s.idx_tup_fetch,\n    pg_size_pretty(pg_relation_size(i.indexname::regclass)) as index_size\nFROM pg_indexes i\nLEFT JOIN pg_stat_user_indexes s ON i.indexname = s.indexname\nWHERE i.tablename = 'customers'\nORDER BY s.idx_scan DESC NULLS LAST;"
        },
        {
          "name": "Index Maintenance",
          "text": "Regular index maintenance is essential for optimal PostgreSQL performance. Learn how to monitor and maintain indexes.\n\n**Why Index Maintenance Matters:**\n- **Prevents Bloat**: Indexes can become fragmented over time, reducing performance\n- **Updates Statistics**: Helps query planner make better decisions\n- **Maintains Performance**: Ensures indexes continue to provide optimal speed\n- **Prevents Degradation**: Regular maintenance prevents gradual performance decline\n\n**Key Maintenance Tasks:**\n- **REINDEX**: Rebuilds indexes to eliminate bloat and fragmentation\n- **ANALYZE**: Updates table statistics for better query planning\n- **Monitoring**: Tracks index usage and performance over time\n\n**Maintenance Benefits:**\n- **Consistent Performance**: Prevents gradual performance degradation\n- **Better Query Planning**: Updated statistics lead to better execution plans\n- **Storage Optimization**: Removes bloat and reclaims space\n- **Predictable Performance**: Regular maintenance ensures consistent response times\n\n**When to Perform Maintenance:**\n- **High-Volume Systems**: After heavy INSERT/UPDATE/DELETE operations\n- **Performance Issues**: When queries become slower than expected\n- **Scheduled Maintenance**: Regular intervals (weekly/monthly)\n- **After Bulk Operations**: Large data imports or updates",
          "code": "-- Rebuild index - Eliminates bloat and fragmentation\n-- CONCURRENTLY: Allows normal operations during rebuild (PostgreSQL 12+)\n-- Use when: Index performance degrades, after heavy modifications\n-- Benefits: Restores optimal performance, removes fragmentation\nREINDEX INDEX CONCURRENTLY idx_customer_email;\nREINDEX TABLE CONCURRENTLY customers;\n\n-- Update table statistics - Critical for query optimization\n-- ANALYZE: Updates statistics for query planner\n-- VERBOSE: Shows detailed progress information\n-- Use when: Query performance degrades, after data changes\n-- Benefits: Better query execution plans, improved performance\nANALYZE customers;\nANALYZE VERBOSE customers;\n\n-- Check index bloat - Monitor index efficiency\n-- Compare index size to table size to identify bloat\n-- High ratio = potential bloat, consider REINDEX\n-- Benefits: Identifies indexes needing maintenance\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexname::regclass)) as index_size,\n    pg_size_pretty(pg_relation_size(tablename::regclass)) as table_size\nFROM pg_indexes \nWHERE tablename = 'customers';\n\n-- Monitor index usage over time - Track performance trends\n-- Creates historical record of index usage\n-- Helps identify performance patterns and degradation\n-- Benefits: Proactive maintenance, performance trend analysis\nCREATE TABLE index_usage_log (\n    log_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    indexname VARCHAR(100),\n    scans BIGINT,\n    reads BIGINT\n);\n\n-- Log current index usage - Capture current state\n-- Run regularly to track usage patterns\n-- Benefits: Historical analysis, maintenance planning\nINSERT INTO index_usage_log (indexname, scans, reads)\nSELECT indexname, idx_scan, idx_tup_read\nFROM pg_stat_user_indexes;\n\n-- Automated index maintenance function - Streamline maintenance\n-- Combines multiple maintenance tasks\n-- Can be scheduled with pg_cron or external tools\n-- Benefits: Consistent maintenance, reduced manual effort\nCREATE OR REPLACE FUNCTION maintain_indexes()\nRETURNS VOID AS $$\nDECLARE\n    rec RECORD;\nBEGIN\n    -- Update statistics for all tables\n    ANALYZE;\n    \n    -- Log maintenance completion\n    INSERT INTO maintenance_log (task, completed_at)\n    VALUES ('index_maintenance', CURRENT_TIMESTAMP);\nEND;\n$$ LANGUAGE plpgsql;"
        }
      ]
    },
    {
      "id": "postgresql-query-optimization",
      "title": "âš¡ PostgreSQL Query Optimization â€” Performance Tuning",
      "sections": [
        {
          "name": "Query Execution Plans",
          "text": "Understanding PostgreSQL query execution plans is crucial for identifying performance bottlenecks and optimizing queries.\n\n**Why Analyze Execution Plans?**\n- **Identify Bottlenecks**: Find slow operations in query execution\n- **Verify Index Usage**: Ensure indexes are being used effectively\n- **Compare Approaches**: Test different query strategies\n- **Optimize Performance**: Make data-driven optimization decisions\n\n**Key EXPLAIN Options:**\n- **ANALYZE**: Shows actual execution time and row counts (requires query execution)\n- **BUFFERS**: Shows disk I/O statistics (buffer hits/misses)\n- **FORMAT JSON**: Machine-readable output for analysis tools\n- **VERBOSE**: Detailed information about each operation\n\n**Common Performance Issues:**\n- **Seq Scan**: Full table scan (slow for large tables)\n- **High Buffer Misses**: Indicates poor caching or missing indexes\n- **Nested Loops**: Can be slow for large datasets\n- **Sort Operations**: Expensive for large result sets\n\n**Optimization Benefits:**\n- **Faster Queries**: Identify and fix performance bottlenecks\n- **Better Resource Usage**: Reduce CPU and I/O overhead\n- **Scalability**: Ensure queries perform well as data grows\n- **Cost Reduction**: Lower infrastructure costs through efficiency",
          "code": "-- Basic execution plan - Shows query structure and estimated costs\n-- Use for: Understanding query flow, identifying expensive operations\n-- Benefits: Quick analysis without executing query\nEXPLAIN SELECT * FROM customers WHERE email = 'john@example.com';\n\n-- Detailed execution plan with timing - Most comprehensive analysis\n-- ANALYZE: Shows actual execution time and row counts\n-- BUFFERS: Shows disk I/O statistics (critical for performance)\n-- FORMAT JSON: Machine-readable for analysis tools\n-- Use for: Detailed performance analysis, identifying bottlenecks\n-- Benefits: Real performance data, I/O analysis, optimization insights\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \nSELECT c.*, o.order_date, o.total_amount\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nWHERE c.status = 'active'\nORDER BY o.order_date DESC\nLIMIT 10;\n\n-- Compare different query approaches - Test optimization strategies\n-- Approach 1: Subquery - Often slower due to repeated execution\n-- Approach 2: JOIN - Usually faster, single execution\n-- Use for: Choosing between different query patterns\n-- Benefits: Data-driven optimization decisions\nEXPLAIN (ANALYZE) \n-- Approach 1: Subquery\nSELECT * FROM customers \nWHERE id IN (SELECT customer_id FROM orders WHERE order_date > '2023-01-01');\n\nEXPLAIN (ANALYZE) \n-- Approach 2: JOIN\nSELECT DISTINCT c.* FROM customers c\nJOIN orders o ON c.id = o.customer_id\nWHERE o.order_date > '2023-01-01';\n\n-- Check query costs - Estimate resource usage\n-- COSTS: Shows estimated costs (useful for optimization)\n-- BUFFERS: Shows buffer usage without executing\n-- Use for: Cost estimation, optimization planning\n-- Benefits: Understand resource requirements before execution\nEXPLAIN (COSTS, BUFFERS)\nSELECT COUNT(*) FROM orders WHERE order_date > '2023-01-01';\n\n-- Analyze parallel query execution - Check parallelization\n-- VERBOSE: Shows detailed parallel execution information\n-- Use for: Understanding parallel query performance\n-- Benefits: Optimize for parallel execution, identify parallel bottlenecks\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT * FROM large_table WHERE condition = 'value';"
        },
        {
          "name": "Common Query Patterns",
          "text": "Learn efficient PostgreSQL query patterns and avoid common performance pitfalls.\n\n**Pagination Patterns:**\n- **OFFSET/LIMIT**: Simple but slow for large offsets (PostgreSQL scans all skipped rows)\n- **Cursor-based**: Much faster for large datasets, uses indexed columns\n\n**Aggregation Benefits:**\n- **DATE_TRUNC**: Efficient grouping by time periods\n- **Window Functions**: Powerful for ranking and analytical queries\n- **CTEs**: Improve readability and enable complex logic\n\n**Performance Advantages:**\n- **Cursor Pagination**: O(log n) vs O(n) for large offsets\n- **Lateral Joins**: Efficient correlated subqueries\n- **Window Functions**: Single pass through data for complex calculations\n- **CTEs**: Reusable query components, better optimization\n\n**When to Use Each Pattern:**\n- **Cursor Pagination**: Large datasets, real-time applications\n- **Window Functions**: Ranking, running totals, analytical queries\n- **Lateral Joins**: Complex correlated subqueries\n- **CTEs**: Multi-step queries, recursive operations",
          "code": "-- Efficient pagination - Basic but can be slow for large offsets\n-- Problem: OFFSET scans all skipped rows (slow for large offsets)\n-- Use for: Small datasets, simple pagination needs\n-- Benefits: Simple to implement, works with any ORDER BY\nSELECT * FROM customers\nORDER BY id\nLIMIT 20 OFFSET 40;\n\n-- Better pagination with cursor - Much faster for large datasets\n-- Solution: Use indexed column values instead of OFFSET\n-- Use for: Large datasets, real-time applications, infinite scroll\n-- Benefits: Consistent performance, O(log n) complexity\nSELECT * FROM customers\nWHERE id > 1000  -- Last seen ID from previous page\nORDER BY id\nLIMIT 20;\n\n-- Efficient aggregation - Time-based grouping\n-- DATE_TRUNC: Groups by time periods efficiently\n-- Use for: Time-series analysis, reporting, dashboards\n-- Benefits: Fast grouping, flexible time periods\nSELECT \n    DATE_TRUNC('month', order_date) as month,\n    COUNT(*) as order_count,\n    SUM(total_amount) as total_revenue\nFROM orders\nWHERE order_date >= '2023-01-01'\nGROUP BY DATE_TRUNC('month', order_date)\nORDER BY month;\n\n-- Window functions for ranking - Powerful analytical queries\n-- ROW_NUMBER: Assigns sequential numbers within partitions\n-- Use for: Top-N queries, ranking, analytical reports\n-- Benefits: Single pass through data, flexible partitioning\nSELECT \n    customer_id,\n    order_date,\n    total_amount,\n    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date DESC) as rn\nFROM orders\nWHERE rn <= 5;  -- Top 5 orders per customer\n\n-- Lateral joins for complex queries - Efficient correlated subqueries\n-- LATERAL: Allows subquery to reference outer query columns\n-- Use for: Complex correlated subqueries, per-row calculations\n-- Benefits: Better performance than correlated subqueries, more readable\nSELECT c.*, recent_orders.order_date, recent_orders.total_amount\nFROM customers c\nCROSS JOIN LATERAL (\n    SELECT order_date, total_amount\n    FROM orders o\n    WHERE o.customer_id = c.id\n    ORDER BY order_date DESC\n    LIMIT 3\n) recent_orders;\n\n-- Common Table Expressions (CTEs) - Reusable query components\n-- WITH: Creates temporary named result sets\n-- Use for: Complex multi-step queries, recursive operations\n-- Benefits: Improved readability, better optimization, reusability\nWITH customer_totals AS (\n    SELECT customer_id, SUM(total_amount) as total_spent\n    FROM orders\n    GROUP BY customer_id\n)\nSELECT c.name, ct.total_spent\nFROM customers c\nJOIN customer_totals ct ON c.id = ct.customer_id\nORDER BY ct.total_spent DESC;"
        },
        {
          "name": "Query Optimization Techniques",
          "text": "Advanced PostgreSQL techniques for optimizing database queries including hints, materialized views, and query rewriting.",
          "code": "-- Use appropriate JOIN types\n-- INNER JOIN for exact matches\nSELECT c.name, o.order_date\nFROM customers c\nINNER JOIN orders o ON c.id = o.customer_id;\n\n-- LEFT JOIN to preserve all customers\nSELECT c.name, COUNT(o.id) as order_count\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.id, c.name;\n\n-- Materialized view for complex aggregations\nCREATE MATERIALIZED VIEW customer_summary AS\nSELECT \n    c.id,\n    c.name,\n    c.email,\n    COUNT(o.id) as total_orders,\n    SUM(o.total_amount) as total_spent,\n    MAX(o.order_date) as last_order_date\nFROM customers c\nLEFT JOIN orders o ON c.id = o.customer_id\nGROUP BY c.id, c.name, c.email;\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW CONCURRENTLY customer_summary;\n\n-- Query hints (PostgreSQL 12+)\n-- Force specific join order\nSELECT /*+ Leading(c o) */ c.*, o.order_date\nFROM customers c\nJOIN orders o ON c.id = o.customer_id;\n\n-- Force specific index usage\nSELECT /*+ IndexScan(customers idx_customer_email) */ *\nFROM customers\nWHERE email = 'john@example.com';\n\n-- Parallel query configuration\nSET max_parallel_workers_per_gather = 4;\nSET parallel_tuple_cost = 0.1;\nSET parallel_setup_cost = 1000.0;"
        }
      ]
    },
    {
      "id": "postgresql-transactions",
      "title": "ðŸ”„ PostgreSQL Transactions & Concurrency â€” ACID Properties",
      "sections": [
        {
          "name": "Transaction Isolation Levels",
          "text": "Understanding PostgreSQL isolation levels and their impact on concurrency and data consistency.",
          "code": "-- Set isolation level\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN;\nSELECT * FROM customers WHERE id = 1;\n-- Other operations...\nCOMMIT;\n\n-- Serializable isolation (highest level)\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\nCOMMIT;\n\n-- Check current isolation level\nSELECT current_setting('transaction_isolation');\n\n-- Deadlock handling\nBEGIN;\nUPDATE customers SET status = 'inactive' WHERE id = 1;\n-- If deadlock occurs, PostgreSQL will rollback and retry\nUPDATE orders SET status = 'cancelled' WHERE customer_id = 1;\nCOMMIT;\n\n-- Transaction with savepoints\nBEGIN;\nINSERT INTO customers (name, email) VALUES ('John', 'john@example.com');\nSAVEPOINT sp1;\nINSERT INTO orders (customer_id, total_amount) VALUES (LASTVAL(), 100);\nROLLBACK TO sp1;  -- Rollback to savepoint\nCOMMIT;\n\n-- Two-phase commit preparation\nBEGIN;\nPREPARE TRANSACTION 'txn_001';\n-- Later: COMMIT PREPARED 'txn_001'; or ROLLBACK PREPARED 'txn_001';"
        },
        {
          "name": "Locking Mechanisms",
          "text": "Understanding PostgreSQL locking mechanisms and how to avoid lock contention and deadlocks.",
          "code": "-- Explicit locking\nBEGIN;\nSELECT * FROM customers WHERE id = 1 FOR UPDATE;\n-- This row is now locked for update\nUPDATE customers SET status = 'active' WHERE id = 1;\nCOMMIT;\n\n-- Row-level locking\nBEGIN;\nSELECT * FROM customers WHERE email = 'john@example.com' FOR SHARE;\n-- Other transactions can read but not modify\nCOMMIT;\n\n-- Advisory locks\nSELECT pg_advisory_lock(12345);\n-- Perform critical section operations\nSELECT pg_advisory_unlock(12345);\n\n-- Check for locks\nSELECT \n    l.locktype,\n    l.database,\n    l.relation,\n    l.page,\n    l.tuple,\n    l.virtualxid,\n    l.transactionid,\n    l.classid,\n    l.objid,\n    l.objsubid,\n    l.virtualtransaction,\n    l.pid,\n    l.mode,\n    l.granted\nFROM pg_locks l\nJOIN pg_stat_activity a ON l.pid = a.pid;\n\n-- Lock timeout\nSET lock_timeout = '5s';\nSELECT * FROM customers WHERE id = 1 FOR UPDATE NOWAIT;\n\n-- Check for blocking queries\nSELECT \n    blocked_locks.pid AS blocked_pid,\n    blocked_activity.usename AS blocked_user,\n    blocking_locks.pid AS blocking_pid,\n    blocking_activity.usename AS blocking_user,\n    blocked_activity.query AS blocked_statement,\n    blocking_activity.query AS current_statement_in_blocking_process\nFROM pg_catalog.pg_locks blocked_locks\nJOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid\nJOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype\n    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database\n    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation\n    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page\n    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple\n    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid\n    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid\n    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid\n    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid\n    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid\n    AND blocking_locks.pid != blocked_locks.pid\nJOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid\nWHERE NOT blocked_locks.granted;"
        },
        {
          "name": "Concurrent Operations",
          "text": "Best practices for handling concurrent PostgreSQL operations and maintaining data consistency.",
          "code": "-- Optimistic concurrency control\nUPDATE customers \nSET status = 'active', version = version + 1\nWHERE id = 1 AND version = 5;\n\n-- Check if update affected any rows\nSELECT ROW_COUNT();\n\n-- Pessimistic locking for critical operations\nBEGIN;\nSELECT * FROM accounts WHERE id = 1 FOR UPDATE NOWAIT;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\nCOMMIT;\n\n-- Batch operations for better concurrency\nINSERT INTO order_items (order_id, product_id, quantity, price)\nVALUES \n    (1, 101, 2, 29.99),\n    (1, 102, 1, 49.99),\n    (1, 103, 3, 19.99);\n\n-- Use UPSERT for concurrent inserts\nINSERT INTO customers (email, name) \nVALUES ('john@example.com', 'John Doe')\nON CONFLICT (email) DO UPDATE SET \n    name = EXCLUDED.name,\n    updated_at = CURRENT_TIMESTAMP;\n\n-- Concurrent data loading\nCOPY customers (name, email, status) FROM '/path/to/data.csv' WITH CSV HEADER;\n\n-- Parallel query execution\nSET max_parallel_workers_per_gather = 4;\nSELECT COUNT(*) FROM large_table WHERE condition = 'value';\n\n-- Connection pooling considerations\n-- Use prepared statements for repeated queries\nPREPARE get_customer AS SELECT * FROM customers WHERE id = $1;\nEXECUTE get_customer(1);\nEXECUTE get_customer(2);\nDEALLOCATE get_customer;"
        }
      ]
    },
    {
      "id": "postgresql-stored-procedures",
      "title": "ðŸ“ PostgreSQL Stored Procedures â€” Business Logic in Database",
      "sections": [
        {
          "name": "Creating Functions",
          "text": "Learn how to create and use PostgreSQL functions to encapsulate business logic in the database.",
          "code": "-- Simple function\nCREATE OR REPLACE FUNCTION get_customer_orders(\n    customer_email VARCHAR(255)\n)\nRETURNS TABLE (\n    order_id INTEGER,\n    order_date DATE,\n    total_amount DECIMAL(10,2)\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT o.id, o.order_date, o.total_amount\n    FROM orders o\n    JOIN customers c ON o.customer_id = c.id\n    WHERE c.email = customer_email\n    ORDER BY o.order_date DESC;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Call the function\nSELECT * FROM get_customer_orders('john@example.com');\n\n-- Function with parameters and return value\nCREATE OR REPLACE FUNCTION calculate_order_total(\n    order_items JSONB\n)\nRETURNS DECIMAL(10,2) AS $$\nDECLARE\n    total DECIMAL(10,2) := 0;\n    item JSONB;\nBEGIN\n    FOR item IN SELECT * FROM jsonb_array_elements(order_items)\n    LOOP\n        total := total + (item->>'quantity')::INTEGER * (item->>'price')::DECIMAL;\n    END LOOP;\n    RETURN total;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Function with default parameters\nCREATE OR REPLACE FUNCTION get_customers(\n    status_filter VARCHAR(20) DEFAULT 'active',\n    limit_count INTEGER DEFAULT 100\n)\nRETURNS TABLE (\n    id INTEGER,\n    name VARCHAR(100),\n    email VARCHAR(255)\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT c.id, c.name, c.email\n    FROM customers c\n    WHERE c.status = status_filter\n    LIMIT limit_count;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Call with default parameters\nSELECT * FROM get_customers();\nSELECT * FROM get_customers('inactive', 50);"
        },
        {
          "name": "Advanced Functions",
          "text": "Advanced PostgreSQL function techniques including error handling, cursors, and dynamic SQL.",
          "code": "-- Function with error handling\nCREATE OR REPLACE FUNCTION transfer_money(\n    from_account INTEGER,\n    to_account INTEGER,\n    amount DECIMAL(10,2)\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    from_balance DECIMAL(10,2);\nBEGIN\n    -- Check if accounts exist and have sufficient funds\n    SELECT balance INTO from_balance\n    FROM accounts\n    WHERE id = from_account;\n    \n    IF from_balance IS NULL THEN\n        RAISE EXCEPTION 'Source account not found';\n    END IF;\n    \n    IF from_balance < amount THEN\n        RAISE EXCEPTION 'Insufficient funds';\n    END IF;\n    \n    -- Perform transfer\n    UPDATE accounts SET balance = balance - amount WHERE id = from_account;\n    UPDATE accounts SET balance = balance + amount WHERE id = to_account;\n    \n    -- Log transaction\n    INSERT INTO transactions (from_account, to_account, amount, timestamp)\n    VALUES (from_account, to_account, amount, CURRENT_TIMESTAMP);\n    \n    RETURN TRUE;\nEXCEPTION\n    WHEN OTHERS THEN\n        ROLLBACK;\n        RAISE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Function with cursors\nCREATE OR REPLACE FUNCTION process_orders_batch(\n    batch_size INTEGER DEFAULT 100\n)\nRETURNS INTEGER AS $$\nDECLARE\n    order_cursor CURSOR FOR \n        SELECT id, customer_id, total_amount \n        FROM orders \n        WHERE status = 'pending'\n        ORDER BY created_at;\n    processed_count INTEGER := 0;\n    order_record RECORD;\nBEGIN\n    OPEN order_cursor;\n    \n    LOOP\n        FETCH order_cursor INTO order_record;\n        EXIT WHEN NOT FOUND;\n        \n        -- Process order\n        UPDATE orders SET status = 'processed' WHERE id = order_record.id;\n        processed_count := processed_count + 1;\n        \n        IF processed_count >= batch_size THEN\n            EXIT;\n        END IF;\n    END LOOP;\n    \n    CLOSE order_cursor;\n    RETURN processed_count;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Dynamic SQL function\nCREATE OR REPLACE FUNCTION execute_dynamic_query(\n    table_name TEXT,\n    where_clause TEXT DEFAULT '1=1'\n)\nRETURNS TABLE (\n    result JSONB\n) AS $$\nDECLARE\n    query_text TEXT;\nBEGIN\n    query_text := format('SELECT to_jsonb(t.*) FROM %I t WHERE %s', table_name, where_clause);\n    \n    RETURN QUERY EXECUTE query_text;\nEND;\n$$ LANGUAGE plpgsql;"
        },
        {
          "name": "Triggers and Event Handling",
          "text": "Use PostgreSQL triggers to automatically execute code when database events occur, maintaining data integrity and audit trails.",
          "code": "-- Audit trigger function\nCREATE OR REPLACE FUNCTION audit_trigger_function()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'INSERT' THEN\n        INSERT INTO audit_log (table_name, operation, old_data, new_data, user_name, timestamp)\n        VALUES (TG_TABLE_NAME, 'INSERT', NULL, row_to_json(NEW), current_user, CURRENT_TIMESTAMP);\n        RETURN NEW;\n    ELSIF TG_OP = 'UPDATE' THEN\n        INSERT INTO audit_log (table_name, operation, old_data, new_data, user_name, timestamp)\n        VALUES (TG_TABLE_NAME, 'UPDATE', row_to_json(OLD), row_to_json(NEW), current_user, CURRENT_TIMESTAMP);\n        RETURN NEW;\n    ELSIF TG_OP = 'DELETE' THEN\n        INSERT INTO audit_log (table_name, operation, old_data, new_data, user_name, timestamp)\n        VALUES (TG_TABLE_NAME, 'DELETE', row_to_json(OLD), NULL, current_user, CURRENT_TIMESTAMP);\n        RETURN OLD;\n    END IF;\n    RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create audit log table\nCREATE TABLE audit_log (\n    id SERIAL PRIMARY KEY,\n    table_name VARCHAR(50),\n    operation VARCHAR(10),\n    old_data JSONB,\n    new_data JSONB,\n    user_name VARCHAR(50),\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create trigger\nCREATE TRIGGER customers_audit_trigger\n    AFTER INSERT OR UPDATE OR DELETE ON customers\n    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();\n\n-- Validation trigger\nCREATE OR REPLACE FUNCTION validate_customer_email()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF NEW.email !~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$' THEN\n        RAISE EXCEPTION 'Invalid email format';\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER validate_email_trigger\n    BEFORE INSERT OR UPDATE ON customers\n    FOR EACH ROW EXECUTE FUNCTION validate_customer_email();\n\n-- Conditional trigger\nCREATE OR REPLACE FUNCTION log_high_value_orders()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF NEW.total_amount > 1000 THEN\n        INSERT INTO high_value_orders_log (order_id, amount, timestamp)\n        VALUES (NEW.id, NEW.total_amount, CURRENT_TIMESTAMP);\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER high_value_order_trigger\n    AFTER INSERT ON orders\n    FOR EACH ROW EXECUTE FUNCTION log_high_value_orders();"
        }
      ]
    },
    {
      "id": "postgresql-security",
      "title": "ðŸ”’ PostgreSQL Security â€” Access Control & Encryption",
      "sections": [
        {
          "name": "User Management & Roles",
          "text": "Implement proper user management and role-based access control for PostgreSQL security.",
          "code": "-- Create roles\nCREATE ROLE db_admin;\nCREATE ROLE db_reader;\nCREATE ROLE db_writer;\n\n-- Grant privileges to roles\nGRANT ALL PRIVILEGES ON DATABASE mydb TO db_admin;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO db_reader;\nGRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO db_writer;\n\n-- Create users and assign roles\nCREATE USER admin_user WITH PASSWORD 'secure_password';\nCREATE USER reader_user WITH PASSWORD 'readonly_password';\nCREATE USER writer_user WITH PASSWORD 'write_password';\n\nGRANT db_admin TO admin_user;\nGRANT db_reader TO reader_user;\nGRANT db_writer TO writer_user;\n\n-- Row-level security\nALTER TABLE customers ENABLE ROW LEVEL SECURITY;\n\nCREATE POLICY customer_access_policy ON customers\n    FOR ALL TO db_writer\n    USING (created_by = current_user);\n\n-- View current users and roles\nSELECT \n    rolname,\n    rolsuper,\n    rolinherit,\n    rolcreaterole,\n    rolcreatedb,\n    rolcanlogin\nFROM pg_roles;\n\n-- Grant schema-level privileges\nGRANT USAGE ON SCHEMA public TO db_reader;\nGRANT CREATE ON SCHEMA public TO db_writer;\n\n-- Grant table-level privileges\nGRANT SELECT ON customers TO db_reader;\nGRANT INSERT, UPDATE ON customers TO db_writer;\n\n-- Revoke privileges\nREVOKE SELECT ON customers FROM db_reader;\n\n-- Create group roles\nCREATE ROLE sales_team;\nCREATE ROLE marketing_team;\n\n-- Grant group privileges\nGRANT SELECT ON customers TO sales_team;\nGRANT SELECT ON orders TO sales_team;\nGRANT SELECT ON customers TO marketing_team;\n\n-- Add users to groups\nGRANT sales_team TO sales_user1, sales_user2;\nGRANT marketing_team TO marketing_user1;"
        },
        {
          "name": "Data Encryption",
          "text": "Implement data encryption at rest and in transit to protect sensitive information in PostgreSQL.",
          "code": "-- Enable SSL connections\n-- In postgresql.conf:\nssl = on\nssl_cert_file = 'server.crt'\nssl_key_file = 'server.key'\nssl_ca_file = 'ca.crt'\n\n-- Encrypt sensitive columns\nCREATE EXTENSION IF NOT EXISTS pgcrypto;\n\n-- Encrypt data on insert\nINSERT INTO users (username, password_hash, encrypted_email)\nVALUES (\n    'john_doe',\n    crypt('password123', gen_salt('bf')),\n    pgp_sym_encrypt('john@example.com', 'encryption_key')\n);\n\n-- Decrypt data on select\nSELECT \n    username,\n    pgp_sym_decrypt(encrypted_email, 'encryption_key') as email\nFROM users\nWHERE username = 'john_doe';\n\n-- Hash passwords\nCREATE OR REPLACE FUNCTION hash_password(password TEXT)\nRETURNS TEXT AS $$\nBEGIN\n    RETURN crypt(password, gen_salt('bf', 12));\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Verify password\nCREATE OR REPLACE FUNCTION verify_password(password TEXT, hash TEXT)\nRETURNS BOOLEAN AS $$\nBEGIN\n    RETURN hash = crypt(password, hash);\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Asymmetric encryption\n-- Generate key pair (outside database)\n-- gpg --gen-key\n\n-- Encrypt with public key\nINSERT INTO sensitive_data (data)\nVALUES (pgp_pub_encrypt('sensitive information', dearmor('-----BEGIN PGP PUBLIC KEY BLOCK-----...')));\n\n-- Decrypt with private key\nSELECT pgp_pub_decrypt(data, dearmor('-----BEGIN PGP PRIVATE KEY BLOCK-----...'))\nFROM sensitive_data;\n\n-- Column-level encryption\nCREATE TABLE encrypted_users (\n    id SERIAL PRIMARY KEY,\n    username VARCHAR(50),\n    encrypted_ssn BYTEA,\n    encryption_key_id INTEGER\n);\n\n-- Encrypt SSN\nINSERT INTO encrypted_users (username, encrypted_ssn, encryption_key_id)\nVALUES ('john', pgp_sym_encrypt('123-45-6789', 'secret_key'), 1);\n\n-- Decrypt SSN\nSELECT username, pgp_sym_decrypt(encrypted_ssn, 'secret_key') as ssn\nFROM encrypted_users;"
        },
        {
          "name": "Security Monitoring",
          "text": "Monitor PostgreSQL security events and implement logging for security auditing.",
          "code": "-- Enable connection logging\n-- In postgresql.conf:\nlog_connections = on\nlog_disconnections = on\nlog_hostname = on\nlog_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '\nlog_statement = 'all'\nlog_min_duration_statement = 1000\n\n-- Create security event log table\nCREATE TABLE security_events (\n    id SERIAL PRIMARY KEY,\n    event_type VARCHAR(50),\n    username VARCHAR(50),\n    database_name VARCHAR(50),\n    client_ip INET,\n    event_details JSONB,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Function to log security events\nCREATE OR REPLACE FUNCTION log_security_event(\n    event_type VARCHAR(50),\n    username VARCHAR(50),\n    details JSONB DEFAULT NULL\n)\nRETURNS VOID AS $$\nBEGIN\n    INSERT INTO security_events (event_type, username, event_details)\n    VALUES (event_type, username, details);\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Monitor failed login attempts\nSELECT \n    username,\n    COUNT(*) as failed_attempts,\n    MAX(timestamp) as last_attempt\nFROM security_events\nWHERE event_type = 'failed_login'\n    AND timestamp > CURRENT_TIMESTAMP - INTERVAL '1 hour'\nGROUP BY username\nHAVING COUNT(*) > 5;\n\n-- Check current connections\nSELECT \n    pid,\n    usename,\n    application_name,\n    client_addr,\n    state,\n    query_start,\n    query\nFROM pg_stat_activity\nWHERE state = 'active';\n\n-- Monitor privilege changes\nCREATE OR REPLACE FUNCTION audit_privilege_changes()\nRETURNS TRIGGER AS $$\nBEGIN\n    INSERT INTO security_events (event_type, username, event_details)\n    VALUES ('privilege_change', current_user, \n        jsonb_build_object(\n            'old_privileges', row_to_json(OLD),\n            'new_privileges', row_to_json(NEW)\n        )\n    );\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create trigger on pg_authid\n-- Note: This requires superuser privileges\n-- CREATE TRIGGER audit_authid_changes\n--     AFTER UPDATE ON pg_authid\n--     FOR EACH ROW EXECUTE FUNCTION audit_privilege_changes();\n\n-- Check for suspicious activity\nSELECT \n    usename,\n    client_addr,\n    COUNT(*) as connection_count,\n    MAX(backend_start) as last_connection\nFROM pg_stat_activity\nWHERE backend_start > CURRENT_TIMESTAMP - INTERVAL '1 hour'\nGROUP BY usename, client_addr\nHAVING COUNT(*) > 10\nORDER BY connection_count DESC;"
        }
      ]
    },
    {
      "id": "postgresql-administration",
      "title": "âš™ï¸ PostgreSQL Administration â€” Maintenance & Monitoring",
      "sections": [
        {
          "name": "Backup & Recovery",
          "text": "Implement comprehensive backup and recovery strategies to protect your PostgreSQL database.",
          "code": "-- Full database backup\npg_dump -h localhost -U postgres -d mydb > backup_$(date +%Y%m%d_%H%M%S).sql\n\n-- Compressed backup\npg_dump -h localhost -U postgres -d mydb | gzip > backup_$(date +%Y%m%d_%H%M%S).sql.gz\n\n-- Schema-only backup\npg_dump -h localhost -U postgres -d mydb --schema-only > schema_backup.sql\n\n-- Data-only backup\npg_dump -h localhost -U postgres -d mydb --data-only > data_backup.sql\n\n-- Restore from backup\npsql -h localhost -U postgres -d mydb < backup_20231201_120000.sql\n\n-- Point-in-time recovery setup\n-- In postgresql.conf:\nwal_level = replica\nmax_wal_senders = 3\nwal_keep_segments = 64\narchive_mode = on\narchive_command = 'cp %p /backup/wal_archive/%f'\n\n-- Create base backup\npg_basebackup -h localhost -U postgres -D /backup/base_backup -Ft -z -P\n\n-- Continuous archiving\n-- In postgresql.conf:\narchive_mode = on\narchive_command = 'test ! -f /backup/wal_archive/%f && cp %p /backup/wal_archive/%f'\n\n-- Restore to point-in-time\n-- 1. Restore base backup\n-- 2. Copy WAL files\n-- 3. Create recovery.conf:\nrestore_command = 'cp /backup/wal_archive/%f %p'\nrecovery_target_time = '2023-12-01 12:00:00'\n\n-- Logical replication setup\n-- On master:\nCREATE PUBLICATION my_publication FOR TABLE customers, orders;\n\n-- On replica:\nCREATE SUBSCRIPTION my_subscription\n    CONNECTION 'host=master_host port=5432 dbname=mydb user=replica_user'\n    PUBLICATION my_publication;"
        },
        {
          "name": "Performance Monitoring",
          "text": "Monitor PostgreSQL performance metrics and identify bottlenecks for optimization.",
          "code": "-- Check database size\nSELECT \n    pg_database.datname,\n    pg_size_pretty(pg_database_size(pg_database.datname)) AS size\nFROM pg_database\nORDER BY pg_database_size(pg_database.datname) DESC;\n\n-- Monitor active connections\nSELECT \n    pid,\n    usename,\n    application_name,\n    client_addr,\n    state,\n    query_start,\n    query\nFROM pg_stat_activity\nWHERE state = 'active';\n\n-- Check slow queries (requires pg_stat_statements)\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\nSELECT \n    query,\n    calls,\n    total_time,\n    mean_time,\n    rows\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- Monitor table statistics\nSELECT \n    schemaname,\n    tablename,\n    n_tup_ins as inserts,\n    n_tup_upd as updates,\n    n_tup_del as deletes,\n    n_live_tup as live_tuples,\n    n_dead_tup as dead_tuples\nFROM pg_stat_user_tables\nORDER BY n_live_tup DESC;\n\n-- Check index usage\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n\n-- Monitor replication lag\nSELECT \n    client_addr,\n    state,\n    sent_lsn,\n    write_lsn,\n    flush_lsn,\n    replay_lsn,\n    write_lag,\n    flush_lag,\n    replay_lag\nFROM pg_stat_replication;\n\n-- Check vacuum statistics\nSELECT \n    schemaname,\n    tablename,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze,\n    vacuum_count,\n    autovacuum_count,\n    analyze_count,\n    autoanalyze_count\nFROM pg_stat_user_tables\nORDER BY last_autovacuum DESC;"
        },
        {
          "name": "Maintenance Tasks",
          "text": "Regular maintenance tasks to keep your PostgreSQL database running optimally.",
          "code": "-- Update table statistics\nANALYZE customers;\nANALYZE orders;\nANALYZE order_items;\n\n-- Vacuum to reclaim space\nVACUUM ANALYZE customers;\nVACUUM FULL orders; -- Use with caution, locks table\n\n-- Reindex to rebuild indexes\nREINDEX INDEX CONCURRENTLY idx_customer_email;\nREINDEX TABLE CONCURRENTLY customers;\n\n-- Check for table bloat\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size,\n    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Automated maintenance script\nCREATE OR REPLACE FUNCTION maintenance_routine()\nRETURNS VOID AS $$\nBEGIN\n    -- Update statistics\n    ANALYZE;\n    \n    -- Vacuum tables with low activity\n    PERFORM pg_stat_reset();\n    \n    -- Log maintenance completion\n    INSERT INTO maintenance_log (task, completed_at)\n    VALUES ('routine_maintenance', CURRENT_TIMESTAMP);\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Check for long-running queries\nSELECT \n    pid,\n    now() - pg_stat_activity.query_start AS duration,\n    query\nFROM pg_stat_activity\nWHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'\n    AND state = 'active';\n\n-- Monitor autovacuum activity\nSELECT \n    schemaname,\n    tablename,\n    last_autovacuum,\n    last_autoanalyze,\n    autovacuum_count,\n    autoanalyze_count\nFROM pg_stat_user_tables\nWHERE last_autovacuum IS NOT NULL\nORDER BY last_autovacuum DESC;\n\n-- Check for unused indexes\nSELECT \n    schemaname,\n    tablename,\n    indexname\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\n    AND schemaname = 'public';\n\n-- Monitor WAL generation\nSELECT \n    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0')) as wal_size;\n\n-- Check for table locks\nSELECT \n    pid,\n    usename,\n    application_name,\n    client_addr,\n    state,\n    query\nFROM pg_stat_activity\nWHERE state = 'active'\n    AND query LIKE '%LOCK%';"
        }
      ]
    }
  ]
}
